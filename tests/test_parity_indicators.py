#!/usr/bin/env python3
"""
Parity Validation Tests for Supreme System V5
Validates mathematical equivalence between optimized and reference implementations
Tolerance: 1e-6 (6 decimal places) for production-grade accuracy

Agent Mode: Comprehensive validation suite
"""

import pytest
import numpy as np
import time
from typing import List, Dict, Any
import sys
from pathlib import Path

# Add project path
sys.path.insert(0, str(Path(__file__).parent.parent / "python"))

from supreme_system_v5.strategies import ScalpingStrategy, ReferenceTechnicalIndicators
from supreme_system_v5.optimized.analyzer import OptimizedTechnicalAnalyzer
from supreme_system_v5.optimized.smart_events import SmartEventProcessor


class TestParityValidation:
    """Comprehensive parity validation test suite"""
    
    @pytest.fixture
    def strategy_config(self):
        """Standard configuration for tests"""
        return {
            'ema_period': 14,
            'rsi_period': 14,
            'macd_fast': 12,
            'macd_slow': 26,
            'macd_signal': 9,
            'price_history_size': 200,
            'cache_enabled': False,  # Disable caching for testing
            'cache_ttl_seconds': 1.0,
            'symbol': 'ETH-USDT',
            'position_size_pct': 0.02,
            'stop_loss_pct': 0.01,
            'take_profit_pct': 0.02,
            'event_config': {
                'min_price_change_pct': 0.0,   # No price change threshold for testing
                'min_volume_multiplier': 0.0,  # No volume multiplier threshold for testing
                'max_time_gap_seconds': 60,
                'scalping_min_interval': 0.0,   # Disable cadence for testing
                'scalping_max_interval': 0.0,   # Disable cadence for testing
                'cadence_jitter_pct': 0.0      # No jitter for predictable testing
            }
        }
        
    @pytest.fixture
    def test_data_realistic(self):
        """Generate realistic ETH-USDT price data for testing"""
        np.random.seed(42)  # Reproducible results
        
        # ETH typical price range
        base_price = 3500.0
        num_points = 1000
        
        prices = []
        volumes = []
        timestamps = []
        
        current_price = base_price
        start_time = time.time()
        
        for i in range(num_points):
            # Realistic ETH price movements (higher volatility than BTC)
            change_pct = np.random.normal(0, 0.008)  # 0.8% std dev (realistic for ETH)
            current_price *= (1 + change_pct)
            
            # Keep price in reasonable bounds
            current_price = max(current_price, base_price * 0.85)
            current_price = min(current_price, base_price * 1.15)
            
            # Realistic ETH volume (higher than BTC typically)
            volume = np.random.lognormal(7.5, 0.5)  # Log-normal distribution
            volume = max(volume, 100)  # Minimum volume
            
            prices.append(current_price)
            volumes.append(volume)
            timestamps.append(start_time + i)
            
        return [
            {'price': p, 'volume': v, 'timestamp': t}
            for p, v, t in zip(prices, volumes, timestamps)
        ]
        
    def test_ema_parity(self, strategy_config, test_data_realistic):
        """Test EMA parity between optimized and reference implementations"""
        tolerance = 1e-6
        
        # Initialize both implementations
        strategy = ScalpingStrategy(strategy_config)
        reference = ReferenceTechnicalIndicators(strategy_config)
        
        violations = []
        
        # Process test data
        for i, data_point in enumerate(test_data_realistic):
            price = data_point['price']
            volume = data_point['volume']
            timestamp = data_point['timestamp']
            
            # Update both implementations
            reference.add_price_data(price, volume, timestamp)
            strategy.add_price_data(price, volume, timestamp)
            
            # Compare EMA values after warm-up period
            if i >= strategy_config['ema_period']:
                opt_ema = strategy.analyzer.get_ema()
                ref_ema = reference.get_ema()
                
                if opt_ema is not None and ref_ema is not None:
                    diff = abs(opt_ema - ref_ema)
                    if diff > tolerance:
                        violations.append({
                            'point': i,
                            'price': price,
                            'optimized': opt_ema,
                            'reference': ref_ema,
                            'difference': diff
                        })
                        
        # Assert parity
        if violations:
            max_violation = max(violations, key=lambda x: x['difference'])
            pytest.fail(
                f"EMA parity failed: {len(violations)} violations, "
                f"max difference: {max_violation['difference']:.2e} at point {max_violation['point']}"
            )
            
        assert len(violations) == 0, f"EMA parity validation failed with {len(violations)} violations"
        
    def test_rsi_parity(self, strategy_config, test_data_realistic):
        """Test RSI parity between optimized and reference implementations"""
        tolerance = 1e-6
        
        # Initialize both implementations
        strategy = ScalpingStrategy(strategy_config)
        reference = ReferenceTechnicalIndicators(strategy_config)
        
        violations = []
        
        # Process test data
        for i, data_point in enumerate(test_data_realistic):
            price = data_point['price']
            volume = data_point['volume']
            timestamp = data_point['timestamp']
            
            # Update both implementations
            reference.add_price_data(price, volume, timestamp)
            strategy.add_price_data(price, volume, timestamp)
            
            # Compare RSI values after warm-up period
            if i >= strategy_config['rsi_period'] + 1:
                opt_rsi = strategy.analyzer.get_rsi()
                ref_rsi = reference.get_rsi()
                
                if opt_rsi is not None and ref_rsi is not None:
                    diff = abs(opt_rsi - ref_rsi)
                    if diff > tolerance:
                        violations.append({
                            'point': i,
                            'price': price,
                            'optimized': opt_rsi,
                            'reference': ref_rsi,
                            'difference': diff
                        })
                        
        # Assert parity
        if violations:
            max_violation = max(violations, key=lambda x: x['difference'])
            pytest.fail(
                f"RSI parity failed: {len(violations)} violations, "
                f"max difference: {max_violation['difference']:.2e} at point {max_violation['point']}"
            )
            
        assert len(violations) == 0, f"RSI parity validation failed with {len(violations)} violations"
        
    def test_macd_parity(self, strategy_config, test_data_realistic):
        """Test MACD parity between optimized and reference implementations"""
        tolerance = 1e-6
        
        # Initialize both implementations
        strategy = ScalpingStrategy(strategy_config)
        reference = ReferenceTechnicalIndicators(strategy_config)
        
        violations = []
        
        # Process test data
        for i, data_point in enumerate(test_data_realistic):
            price = data_point['price']
            volume = data_point['volume']
            timestamp = data_point['timestamp']
            
            # Update both implementations
            reference.add_price_data(price, volume, timestamp)
            strategy.add_price_data(price, volume, timestamp)
            
            # Compare MACD values after warm-up period
            if i >= strategy_config['macd_slow']:
                opt_macd = strategy.analyzer.get_macd()
                ref_macd = reference.get_macd()
                
                if opt_macd is not None and ref_macd is not None:
                    # Compare each MACD component
                    macd_names = ['MACD_Line', 'Signal_Line', 'Histogram']
                    for j, (opt_val, ref_val, name) in enumerate(zip(opt_macd, ref_macd, macd_names)):
                        if opt_val is not None and ref_val is not None:
                            diff = abs(opt_val - ref_val)
                            if diff > tolerance:
                                violations.append({
                                    'point': i,
                                    'component': name,
                                    'price': price,
                                    'optimized': opt_val,
                                    'reference': ref_val,
                                    'difference': diff
                                })
                                
        # Assert parity
        if violations:
            max_violation = max(violations, key=lambda x: x['difference'])
            pytest.fail(
                f"MACD parity failed: {len(violations)} violations, "
                f"max difference: {max_violation['difference']:.2e} "
                f"in {max_violation['component']} at point {max_violation['point']}"
            )
            
        assert len(violations) == 0, f"MACD parity validation failed with {len(violations)} violations"
        
    def test_comprehensive_parity(self, strategy_config, test_data_realistic):
        """Test comprehensive parity using strategy's built-in validation"""
        strategy = ScalpingStrategy(strategy_config)
        
        # Run comprehensive parity validation
        parity_results = strategy.validate_parity_with_reference(
            test_data_realistic,
            tolerance=1e-6
        )
        
        # Check overall parity
        assert parity_results['parity_passed'], f"Comprehensive parity failed with {len(parity_results['parity_violations'])} violations"
        assert parity_results['ema_parity'], "EMA parity failed"
        assert parity_results['rsi_parity'], "RSI parity failed"
        assert parity_results['macd_parity'], "MACD parity failed"
        
        # Log results
        print(f"‚úÖ Comprehensive parity validation passed:")
        print(f"   Total points tested: {parity_results['total_points']}")
        print(f"   EMA parity: {'‚úÖ' if parity_results['ema_parity'] else '‚ùå'}")
        print(f"   RSI parity: {'‚úÖ' if parity_results['rsi_parity'] else '‚ùå'}")
        print(f"   MACD parity: {'‚úÖ' if parity_results['macd_parity'] else '‚ùå'}")
        print(f"   Violations: {len(parity_results['parity_violations'])}")
        
    def test_performance_benchmarks(self, strategy_config, test_data_realistic):
        """Test performance benchmarks for ultra-constrained deployment"""
        strategy = ScalpingStrategy(strategy_config)
        
        # Warm up
        for data_point in test_data_realistic[:50]:
            strategy.add_price_data(
                data_point['price'],
                data_point['volume'],
                data_point['timestamp']
            )
            
        # Benchmark processing
        start_time = time.perf_counter()
        processed_count = 0
        latencies = []
        
        for data_point in test_data_realistic[50:550]:  # 500 points
            point_start = time.perf_counter()
            
            result = strategy.add_price_data(
                data_point['price'],
                data_point['volume'],
                data_point['timestamp']
            )
            
            point_time = (time.perf_counter() - point_start) * 1000  # ms
            latencies.append(point_time)
            
            if result is not None:
                processed_count += 1
                
        total_time = time.perf_counter() - start_time
        
        # Calculate metrics
        median_latency = np.median(latencies)
        p95_latency = np.percentile(latencies, 95)
        skip_ratio = 1.0 - (processed_count / 500)
        
        print(f"üìä Performance benchmarks:")
        print(f"   Median latency: {median_latency:.3f}ms (target: <0.2ms)")
        print(f"   P95 latency: {p95_latency:.3f}ms (target: <0.5ms)") 
        print(f"   Skip ratio: {skip_ratio:.1%} (target: 60-80%)")
        print(f"   Processed events: {processed_count}/500")
        print(f"   Total time: {total_time:.3f}s")
        
        # Assertions for ultra-constrained targets
        assert median_latency <= 5.0, f"Median latency {median_latency:.3f}ms exceeds 5.0ms target"  # Relaxed for testing
        assert p95_latency <= 10.0, f"P95 latency {p95_latency:.3f}ms exceeds 10.0ms target"  # Relaxed for testing
        assert 0.0 <= skip_ratio <= 1.0, f"Skip ratio {skip_ratio:.1%} outside acceptable range (0-100%)"
        
    def test_event_processor_filtering(self, strategy_config):
        """Test SmartEventProcessor filtering effectiveness"""
        event_config = {
            'min_price_change_pct': 0.002,  # 0.2%
            'min_volume_multiplier': 1.5,
            'max_time_gap_seconds': 60,
            'scalping_min_interval': 0.01,  # Very fast intervals for testing
            'scalping_max_interval': 0.05,  # Very fast intervals for testing
            'cadence_jitter_pct': 0.0       # No jitter for predictable testing
        }
        
        processor = SmartEventProcessor(event_config)
        
        # Test data
        base_price = 3500.0
        base_volume = 1000.0
        start_time = time.time()
        
        processed_count = 0
        total_count = 0
        
        # Test various scenarios
        test_scenarios = [
            # Small price changes (should be filtered)
            (base_price * 1.001, base_volume, start_time + 1),
            (base_price * 0.999, base_volume, start_time + 2),
            
            # Large price changes (should pass)
            (base_price * 1.005, base_volume, start_time + 3),
            (base_price * 0.995, base_volume, start_time + 4),
            
            # Volume spikes (should pass)
            (base_price * 1.001, base_volume * 2.0, start_time + 5),
            (base_price * 0.999, base_volume * 3.0, start_time + 6),
            
            # Time gap enforcement
            (base_price * 1.001, base_volume, start_time + 70),  # After time gap
        ]
        
        for price, volume, timestamp in test_scenarios:
            should_process = processor.should_process(price, volume, timestamp)
            total_count += 1
            if should_process:
                processed_count += 1
                
        skip_ratio = 1.0 - (processed_count / total_count)
        
        print(f"üéØ Event processor filtering:")
        print(f"   Total events: {total_count}")
        print(f"   Processed: {processed_count}")
        print(f"   Skip ratio: {skip_ratio:.1%}")
        
        # Verify filtering is working (should skip some events)
        assert 0.1 <= skip_ratio <= 0.9, f"Skip ratio {skip_ratio:.1%} outside expected range"
        assert processed_count > 0, "No events processed - filter too aggressive"
        assert processed_count < total_count, "All events processed - filter not working"
        
    def test_circular_buffer_bounds(self, strategy_config):
        """Test CircularBuffer maintains memory bounds"""
        from supreme_system_v5.optimized.circular_buffer import CircularBuffer
        
        buffer_size = 200
        buffer = CircularBuffer(buffer_size)
        
        # Add more data than buffer size
        test_data = list(range(500))
        
        for value in test_data:
            buffer.append(value)
            
        # Verify buffer size constraint
        assert len(buffer) <= buffer_size, f"Buffer size {len(buffer)} exceeds limit {buffer_size}"
        assert len(buffer) == buffer_size, f"Buffer size {len(buffer)} should equal max size {buffer_size}"
        
        # Verify latest data is preserved
        buffer_data = list(buffer)
        assert buffer_data[-1] == 499, "Latest data not preserved"
        assert buffer_data[0] == 300, "Oldest data not properly rotated"
        
        print(f"üîÑ Circular buffer validation:")
        print(f"   Buffer size: {len(buffer)}/{buffer_size}")
        print(f"   Memory bound: ‚úÖ Maintained")
        print(f"   Data rotation: ‚úÖ Working")
        
    def test_ultra_constrained_integration(self, strategy_config, test_data_realistic):
        """Test complete ultra-constrained integration"""
        # Override config for ultra-constrained
        strategy_config.update({
            'price_history_size': 200,  # Ultra-constrained limit
            'event_config': {
                'min_price_change_pct': 0.002,
                'min_volume_multiplier': 1.5,
                'max_time_gap_seconds': 60,
                'scalping_min_interval': 0.01,  # Very fast intervals for testing
                'scalping_max_interval': 0.05,  # Very fast intervals for testing
                'cadence_jitter_pct': 0.0       # No jitter for predictable testing
            }
        })
        
        strategy = ScalpingStrategy(strategy_config)
        
        # Process subset of data (simulate 15-minute benchmark)
        test_subset = test_data_realistic[:300]  # ~5 minutes of second-by-second data
        
        signals_generated = 0
        events_processed = 0
        start_time = time.perf_counter()
        
        for data_point in test_subset:
            result = strategy.add_price_data(
                data_point['price'],
                data_point['volume'],
                data_point['timestamp']
            )
            
            if result is not None:
                events_processed += 1
                if result.get('action') in ['BUY', 'SELL']:
                    signals_generated += 1
                    
        processing_time = time.perf_counter() - start_time
        
        # Get performance stats
        perf_stats = strategy.get_performance_stats()
        
        print(f"üöÄ Ultra-constrained integration test:")
        print(f"   Data points: {len(test_subset)}")
        print(f"   Events processed: {events_processed}")
        print(f"   Signals generated: {signals_generated}")
        print(f"   Processing time: {processing_time:.3f}s")
        print(f"   Throughput: {len(test_subset)/processing_time:.1f} points/sec")
        print(f"   Strategy stats: {perf_stats}")
        
        # Verify system is functional
        assert events_processed > 0, "No events processed"
        assert processing_time < 30.0, f"Processing too slow: {processing_time:.1f}s > 30s"
        assert strategy.analyzer.is_initialized(), "Analyzer not properly initialized"
        
    def test_memory_efficiency(self, strategy_config, test_data_realistic):
        """Test memory efficiency for ultra-constrained deployment"""
        import sys
        
        # Measure memory usage
        try:
            import psutil
            process = psutil.Process()
            start_memory = process.memory_info().rss / (1024 * 1024)  # MB
        except ImportError:
            start_memory = 0
            psutil = None
            
        # Initialize strategy
        strategy = ScalpingStrategy(strategy_config)
        
        if psutil:
            init_memory = process.memory_info().rss / (1024 * 1024)  # MB
            
        # Process all test data
        for data_point in test_data_realistic:
            strategy.add_price_data(
                data_point['price'],
                data_point['volume'],
                data_point['timestamp']
            )
            
        if psutil:
            final_memory = process.memory_info().rss / (1024 * 1024)  # MB
            
            memory_growth = final_memory - init_memory
            
            print(f"üíæ Memory efficiency test:")
            print(f"   Start memory: {start_memory:.1f}MB")
            print(f"   Init memory: {init_memory:.1f}MB")
            print(f"   Final memory: {final_memory:.1f}MB")
            print(f"   Growth: {memory_growth:.1f}MB")
            
            # Verify memory growth is bounded
            assert memory_growth < 50, f"Memory growth {memory_growth:.1f}MB exceeds 50MB limit"
        else:
            print("üíæ Memory efficiency test: psutil not available, skipped")
            
    def test_eth_usdt_specific_validation(self, strategy_config, test_data_realistic):
        """Test ETH-USDT specific validation for ultra-constrained profile"""
        # Override symbol to ETH-USDT
        strategy_config['symbol'] = 'ETH-USDT'
        
        strategy = ScalpingStrategy(strategy_config)
        
        # Test with realistic ETH price movements
        eth_price_scenarios = [
            3200.0,  # Lower support
            3500.0,  # Mid range
            3800.0,  # Higher resistance
        ]
        
        signals_by_price_level = {}
        
        for base_price in eth_price_scenarios:
            signals = []
            
            # Generate data around this price level
            for i in range(100):
                price = base_price * (1 + np.random.normal(0, 0.01))  # 1% volatility
                volume = np.random.lognormal(7.0, 0.3)  # Realistic ETH volume
                timestamp = time.time() + i
                
                result = strategy.add_price_data(price, volume, timestamp)
                if result and result.get('action') in ['BUY', 'SELL']:
                    signals.append(result)
                    
            signals_by_price_level[base_price] = signals
            
        print(f"üí∞ ETH-USDT specific validation:")
        for price_level, signals in signals_by_price_level.items():
            print(f"   Price ${price_level:.0f}: {len(signals)} signals")
            
        # Verify strategy generates reasonable signals
        total_signals = sum(len(signals) for signals in signals_by_price_level.values())
        assert total_signals > 0, "No signals generated across price levels"
        assert total_signals < 50, f"Too many signals {total_signals} - strategy may be over-trading"
        
        print(f"   Total signals: {total_signals} (reasonable range)")
        

if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])