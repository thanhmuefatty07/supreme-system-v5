name: CI/CD Pipeline

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:

jobs:
  quality-checks:
    name: Quality Checks
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.10', '3.11', '3.12']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'

      - name: Install system dependencies
        run: |
          sudo apt-get update
          sudo apt-get install -y jq

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Cache pre-commit
        uses: actions/cache@v4
        with:
          path: ~/.cache/pre-commit
          key: pre-commit-${{ matrix.python-version }}-${{ hashFiles('.pre-commit-config.yaml') }}

      - name: Run pre-commit hooks
        run: |
          pre-commit install
          pre-commit run --all-files --show-diff-on-failure

      - name: Lint with flake8
        run: |
          flake8 src/ tests/ --count --show-source --statistics
          flake8 src/ tests/ --count --exit-zero --max-complexity=10

      - name: Type check with mypy
        run: mypy src/ --ignore-missing-imports --no-error-summary

      - name: Security scan with bandit
        run: |
          bandit -r src/ -f json -o bandit-report.json || true
          # Check for high/critical severity issues
          if [ -f bandit-report.json ]; then
            high_critical_count=$(jq '.results | map(select(.issue_severity == "HIGH" or .issue_severity == "CRITICAL")) | length' bandit-report.json)
            if [ "$high_critical_count" -gt 0 ]; then
              echo "Found $high_critical_count high/critical security issues"
              jq '.results[] | select(.issue_severity == "HIGH" or .issue_severity == "CRITICAL") | {filename, line_number, issue_text, issue_severity}' bandit-report.json
              exit 1
            fi
          fi

      - name: Dependency vulnerability scan
        run: |
          pip install pip-audit
          pip-audit --requirement requirements.txt --format json || true

      - name: Test with pytest
        run: |
          pytest tests/ \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-fail-under=80 \
            --cov-branch \
            -v \
            --strict-markers \
            --tb=short \
            --junitxml=junit/test-results.xml

      - name: Advanced Testing Suite
        run: |
          # Property-based testing with Hypothesis
          echo "üß™ Running property-based tests..."
          pytest tests/ -k "property" --hypothesis-show-statistics -x || echo "Property tests completed with some failures"

          # Mutation testing sample
          echo "üß¨ Running mutation testing sample..."
          python -c "
          try:
              from src.utils.mutation_tester import run_mutation_analysis
              result = run_mutation_analysis('src/utils/data_utils.py')
              print(f'‚úÖ Mutation Score: {result.mutation_score:.1f}%')
              print(f'üíÄ Killed mutants: {result.killed_mutants}/{result.total_mutants}')
          except Exception as e:
              print(f'‚ö†Ô∏è  Mutation testing failed: {e}')
          " || echo "Mutation testing completed"

          # Chaos engineering sample
          echo "üé≠ Running chaos engineering sample..."
          python -c "
          import asyncio
          from src.utils.chaos_engineer import run_specific_chaos_experiment

          async def test_chaos():
              try:
                  result = await run_specific_chaos_experiment('network_partition')
                  print(f'‚úÖ Chaos test completed: Success={result.success}')
              except Exception as e:
                  print(f'‚ö†Ô∏è  Chaos testing failed: {e}')

          asyncio.run(test_chaos())
          " || echo "Chaos testing completed"

      - name: Coverage Quality Analysis
        run: |
          # Check coverage quality metrics
          python -c "
          import coverage
          import json

          # Load coverage data
          cov = coverage.Coverage()
          cov.load()

          # Analyze coverage quality
          analysis = {}
          for filename in cov.get_data().measured_files():
              if filename.startswith('src/'):
                  file_analysis = cov._analyze(filename)
                  analysis[filename] = {
                      'statements': len(file_analysis.executable),
                      'missing': len(file_analysis.missing),
                      'coverage': file_analysis.percentage
                  }

          # Save analysis
          with open('coverage_quality.json', 'w') as f:
              json.dump(analysis, f, indent=2)

          # Print summary
          total_statements = sum(f['statements'] for f in analysis.values())
          total_missing = sum(f['missing'] for f in analysis.values())

          if total_statements > 0:
              overall_coverage = ((total_statements - total_missing) / total_statements) * 100
              print(f'üìä Overall Coverage: {overall_coverage:.2f}%')
              print(f'üìù Total Statements: {total_statements}')
              print(f'‚ùå Missing Statements: {total_missing}')
          "

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: codecov-umbrella
          fail_ci_if_error: false

      - name: Upload test results
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.python-version }}
          path: |
            junit/
            coverage.xml
            htmlcov/
            bandit-report.json
        if: always()

  docker-build:
    name: Docker Build & Test
    runs-on: ubuntu-latest
    needs: quality-checks
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: actions/docker/setup-buildx-action@v3

      - name: Build Docker image
        uses: actions/docker/build-push-action@v5
        with:
          context: .
          push: false
          tags: supreme-system-v5:${{ github.sha }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Test Docker image
        run: |
          docker run --rm supreme-system-v5:${{ github.sha }} python -c "
          import sys
          sys.path.insert(0, '.')
          import src
          print('‚úÖ Docker image test passed - all imports successful')
          "

      - name: Test CLI in Docker
        run: |
          docker run --rm supreme-system-v5:${{ github.sha }} python -m src.cli --help

  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    needs: quality-checks
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install -r requirements-dev.txt

      - name: Run performance benchmarks
        run: |
          python -m pytest tests/unit/test_strategies_comprehensive.py::TestPerformanceBenchmarks -v --tb=short

      - name: Memory profiling
        run: |
          python -c "
          import tracemalloc
          import pandas as pd
          import numpy as np

          tracemalloc.start()

          # Simulate large data processing
          data = pd.DataFrame({
              'close': np.random.uniform(100, 110, 100000),
              'volume': np.random.randint(1000, 5000, 100000)
          })

          # Calculate some indicators
          data['sma'] = data['close'].rolling(20).mean()
          data['returns'] = data['close'].pct_change()

          current, peak = tracemalloc.get_traced_memory()
          print(f'Current memory usage: {current / 1024 / 1024:.2f} MB')
          print(f'Peak memory usage: {peak / 1024 / 1024:.2f} MB')

          tracemalloc.stop()
          "

  security-audit:
    name: Security Audit
    runs-on: ubuntu-latest
    needs: quality-checks
    if: github.ref == 'refs/heads/main' || github.event_name == 'workflow_dispatch'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@master
        with:
          scan-type: 'fs'
          scan-ref: '.'
          format: 'sarif'
          output: 'trivy-results.sarif'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: 'trivy-results.sarif'

  deploy-staging:
    name: Deploy to Staging
    runs-on: ubuntu-latest
    needs: [quality-checks, docker-build]
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'
    environment: staging

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Deploy to staging environment
        run: |
          echo "üöÄ Deploying to staging environment..."
          echo "Version: ${{ github.sha }}"
          echo "Branch: ${{ github.ref }}"
          # Add actual deployment commands here
          echo "‚úÖ Staging deployment completed"

  notify:
    name: Notification
    runs-on: ubuntu-latest
    needs: [quality-checks, docker-build, performance-test, security-audit]
    if: always()

    steps:
      - name: Notify on failure
        if: failure()
        run: |
          echo "‚ùå CI/CD Pipeline failed"
          echo "Check the logs for details"
          # Add notification logic here (Slack, Discord, etc.)

      - name: Notify on success
        if: success()
        run: |
          echo "‚úÖ CI/CD Pipeline completed successfully"
          echo "All quality checks passed"
          # Add success notification logic here