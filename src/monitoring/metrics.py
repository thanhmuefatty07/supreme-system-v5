"""
ðŸ“ˆ Supreme System V5 - Prometheus Metrics Exporter
Core metrics collection and export for monitoring

Features:
- 8 Tier-1 core metrics as specified
- Real-time metrics collection
- Prometheus format export
- Performance tracking
- Alert-ready thresholds
- Grafana dashboard compatibility
"""

import asyncio
import logging
import time
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional

try:
    from prometheus_client import (
        CONTENT_TYPE_LATEST,
        CollectorRegistry,
        Counter,
        Gauge,
        Histogram,
        Summary,
        generate_latest,
        start_http_server,
    )

    PROMETHEUS_AVAILABLE = True
except ImportError:  # pragma: no cover - graceful degradation path
    PROMETHEUS_AVAILABLE = False

    # Mock classes for graceful degradation
    class Gauge:  # type: ignore
        def __init__(self, *args, **kwargs):
            pass

        def set(self, value):
            pass

        def inc(self, amount=1):
            pass

        def labels(self, *args, **kwargs):  # noqa: D401 - mimic prometheus API
            return self

    class Counter:  # type: ignore
        def __init__(self, *args, **kwargs):
            pass

        def inc(self, amount=1):
            pass

        def labels(self, *args, **kwargs):
            return self

    class Histogram:  # type: ignore
        def __init__(self, *args, **kwargs):
            pass

        def observe(self, value):
            pass

        def labels(self, *args, **kwargs):
            return self

    def start_http_server(port, registry=None):  # type: ignore
        return None

    def generate_latest(registry=None):  # type: ignore
        return b"# Prometheus not available\n"


logger = logging.getLogger("supreme_metrics")


class MetricType(Enum):
    GAUGE = "gauge"
    COUNTER = "counter"
    HISTOGRAM = "histogram"
    SUMMARY = "summary"


@dataclass
class MetricDefinition:
    name: str
    description: str
    metric_type: MetricType
    labels: List[str] = field(default_factory=list)
    buckets: Optional[List[float]] = None


class PrometheusExporter:
    """Prometheus metrics exporter for Supreme System V5"""

    def __init__(self, port: int = 9090):
        self.port = port
        self.registry = CollectorRegistry() if PROMETHEUS_AVAILABLE else None
        self.metrics: Dict[str, Any] = {}
        self.is_running = False
        self.server_started = False
        self.last_update = time.time()

        # Initialize Tier-1 core metrics
        self._initialize_core_metrics()

        logger.info("ðŸ“ˆ Prometheus exporter initialized on port %d", port)
        if not PROMETHEUS_AVAILABLE:
            logger.warning("âš ï¸ Prometheus client not available - metrics will be mocked")

    def _initialize_core_metrics(self) -> None:
        """Initialize the 8 Tier-1 core metrics"""

        # Define core metrics according to requirements
        core_metric_definitions: Dict[str, MetricDefinition] = {
            "api_latency_ms": MetricDefinition(
                name="supreme_api_latency_milliseconds",
                description="API endpoint response latency in milliseconds",
                metric_type=MetricType.HISTOGRAM,
                labels=["endpoint", "method"],
                buckets=[1, 5, 10, 25, 50, 100, 250, 500, 1000],
            ),
            "websocket_clients": MetricDefinition(
                name="supreme_websocket_clients_total",
                description="Number of active WebSocket connections",
                metric_type=MetricType.GAUGE,
            ),
            "trading_loop_ms": MetricDefinition(
                name="supreme_trading_loop_milliseconds",
                description="Trading loop execution time in milliseconds",
                metric_type=MetricType.HISTOGRAM,
                buckets=[1, 5, 10, 25, 50, 100, 250, 500],
            ),
            "orders_executed": MetricDefinition(
                name="supreme_orders_executed_total",
                description="Total number of orders executed",
                metric_type=MetricType.COUNTER,
                labels=["symbol", "side", "status"],
            ),
            "pnl_daily": MetricDefinition(
                name="supreme_pnl_daily_usd",
                description="Daily profit and loss in USD",
                metric_type=MetricType.GAUGE,
                labels=["date"],
            ),
            "exchange_connectivity": MetricDefinition(
                name="supreme_exchange_connectivity",
                description="Exchange connection status (1=connected, 0=disconnected)",
                metric_type=MetricType.GAUGE,
                labels=["exchange"],
            ),
            "gross_exposure_usd": MetricDefinition(
                name="supreme_gross_exposure_usd",
                description="Total gross exposure in USD",
                metric_type=MetricType.GAUGE,
            ),
            "max_drawdown_pct": MetricDefinition(
                name="supreme_max_drawdown_percent",
                description="Maximum drawdown percentage",
                metric_type=MetricType.GAUGE,
                labels=["period"],
            ),
        }

        # Create Prometheus metrics
        for key, definition in core_metric_definitions.items():
            try:
                if definition.metric_type == MetricType.GAUGE:
                    metric = Gauge(
                        definition.name,
                        definition.description,
                        labelnames=definition.labels,
                        registry=self.registry,
                    )
                elif definition.metric_type == MetricType.COUNTER:
                    metric = Counter(
                        definition.name,
                        definition.description,
                        labelnames=definition.labels,
                        registry=self.registry,
                    )
                elif definition.metric_type == MetricType.HISTOGRAM:
                    metric = Histogram(
                        definition.name,
                        definition.description,
                        labelnames=definition.labels,
                        buckets=definition.buckets,
                        registry=self.registry,
                    )
                else:
                    metric = Summary(  # kept for completeness if needed later
                        definition.name,
                        definition.description,
                        labelnames=definition.labels,
                        registry=self.registry,
                    )

                self.metrics[key] = metric
                logger.debug("Initialized metric: %s", definition.name)

            except Exception as exc:  # pragma: no cover
                logger.error("Failed to initialize metric %s: %s", key, exc)
                # Create mock metric for graceful degradation
                self.metrics[key] = type(
                    "MockMetric",
                    (),
                    {
                        "set": lambda self, *args, **kwargs: None,
                        "inc": lambda self, *args, **kwargs: None,
                        "observe": lambda self, *args, **kwargs: None,
                        "labels": lambda *args, **kwargs: self,
                    },
                )()

    async def start_server(self) -> None:
        """Start Prometheus HTTP server"""
        if self.server_started or not PROMETHEUS_AVAILABLE:
            return

        try:
            start_http_server(self.port, registry=self.registry)
            self.server_started = True
            self.is_running = True
            logger.info("âœ… Prometheus server started on port %d", self.port)
            logger.info("   Metrics endpoint: http://localhost:%d/metrics", self.port)
        except Exception as exc:  # pragma: no cover
            logger.error("âŒ Failed to start Prometheus server: %s", exc)

    def stop_server(self) -> None:
        """Stop metrics collection"""
        self.is_running = False
        logger.info("ðŸ›‘ Prometheus metrics collection stopped")

    # Tier-1 Core Metrics Update Methods

    def update_api_latency(self, endpoint: str, method: str, latency_ms: float) -> None:
        """Update API latency metric"""
        try:
            self.metrics["api_latency_ms"].labels(endpoint=endpoint, method=method).observe(
                latency_ms
            )
        except Exception as exc:
            logger.debug("API latency update failed: %s", exc)

    def update_websocket_clients(self, count: int) -> None:
        """Update WebSocket client count"""
        try:
            self.metrics["websocket_clients"].set(count)
        except Exception as exc:
            logger.debug("WebSocket clients update failed: %s", exc)

    def update_trading_loop_time(self, duration_ms: float) -> None:
        """Update trading loop execution time"""
        try:
            self.metrics["trading_loop_ms"].observe(duration_ms)
        except Exception as exc:
            logger.debug("Trading loop time update failed: %s", exc)

    def increment_orders_executed(
        self, symbol: str, side: str, status: str, count: int = 1
    ) -> None:
        """Increment orders executed counter"""
        try:
            self.metrics["orders_executed"].labels(
                symbol=symbol, side=side, status=status
            ).inc(count)
        except Exception as exc:
            logger.debug("Orders executed update failed: %s", exc)

    def update_pnl_daily(self, pnl_usd: float, date: Optional[str] = None) -> None:
        """Update daily PnL"""
        if date is None:
            date = datetime.utcnow().strftime("%Y-%m-%d")
        try:
            self.metrics["pnl_daily"].labels(date=date).set(pnl_usd)
        except Exception as exc:
            logger.debug("Daily PnL update failed: %s", exc)

    def update_exchange_connectivity(self, exchange: str, connected: bool) -> None:
        """Update exchange connectivity status"""
        try:
            self.metrics["exchange_connectivity"].labels(exchange=exchange).set(
                1 if connected else 0
            )
        except Exception as exc:
            logger.debug("Exchange connectivity update failed: %s", exc)

    def update_gross_exposure(self, exposure_usd: float) -> None:
        """Update gross exposure"""
        try:
            self.metrics["gross_exposure_usd"].set(exposure_usd)
        except Exception as exc:
            logger.debug("Gross exposure update failed: %s", exc)

    def update_max_drawdown(self, drawdown_pct: float, period: str = "daily") -> None:
        """Update maximum drawdown percentage"""
        try:
            self.metrics["max_drawdown_pct"].labels(period=period).set(drawdown_pct)
        except Exception as exc:
            logger.debug("Max drawdown update failed: %s", exc)

    def get_metrics_text(self) -> str:
        """Get metrics in Prometheus text format"""
        if not PROMETHEUS_AVAILABLE or not self.registry:
            return "# Prometheus not available\n"

        try:
            return generate_latest(self.registry).decode("utf-8")
        except Exception as exc:
            logger.error("Failed to generate metrics: %s", exc)
            return f"# Error generating metrics: {exc}\n"

    def get_metrics_summary(self) -> Dict[str, Any]:
        """Get summary of current metrics"""
        return {
            "prometheus_available": PROMETHEUS_AVAILABLE,
            "server_running": self.is_running,
            "server_port": self.port,
            "metrics_count": len(self.metrics),
            "core_metrics": list(self.metrics.keys()),
            "last_update": datetime.fromtimestamp(self.last_update).isoformat(),
            "uptime_seconds": time.time() - self.last_update if self.is_running else 0,
        }


class MetricsCollector:
    """Automated metrics collection from system components"""

    def __init__(self, exporter: PrometheusExporter):
        self.exporter = exporter
        self.collection_interval = 15.0  # 15 seconds
        self.is_collecting = False
        self.collection_task: Optional[asyncio.Task] = None

        # Metrics cache for performance
        self.metrics_cache: Dict[str, deque] = defaultdict(lambda: deque(maxlen=100))

        logger.info("ðŸ“ˆ Metrics collector initialized")

    async def start_collection(self) -> None:
        """Start automated metrics collection"""
        if self.is_collecting:
            return

        self.is_collecting = True
        self.collection_task = asyncio.create_task(self._collection_loop())
        logger.info(
            "âœ… Metrics collection started (interval: %ss)", self.collection_interval
        )

    def stop_collection(self) -> None:
        """Stop automated metrics collection"""
        self.is_collecting = False
        if self.collection_task:
            self.collection_task.cancel()
        logger.info("ðŸ›‘ Metrics collection stopped")

    async def _collection_loop(self) -> None:
        """Main collection loop"""
        while self.is_collecting:
            try:
                await self.collect_system_metrics()
                await asyncio.sleep(self.collection_interval)
            except asyncio.CancelledError:
                break
            except Exception as exc:
                logger.error("Metrics collection error: %s", exc)
                await asyncio.sleep(self.collection_interval)

    async def collect_system_metrics(self) -> None:
        """Collect metrics from system components"""
        try:
            # WebSocket clients (from global handler)
            try:
                from ..api.websocket import websocket_handler

                ws_stats = websocket_handler.get_connection_stats()
                self.exporter.update_websocket_clients(ws_stats["active_connections"])
            except Exception:
                pass

            # Trading engine metrics (if available)
            try:
                from ..trading.engine import trading_engine

                if trading_engine and hasattr(trading_engine, "performance_metrics"):
                    metrics = trading_engine.performance_metrics  # noqa: F841 - reserved for future mapping

                    # Update daily PnL if portfolio available
                    if hasattr(trading_engine, "portfolio") and trading_engine.portfolio:
                        pnl = trading_engine.portfolio.daily_pnl
                        self.exporter.update_pnl_daily(pnl)

                        # Calculate gross exposure
                        total_value = (
                            trading_engine.portfolio.get_portfolio_summary().get(
                                "total_value_usd", 0
                            )
                        )
                        self.exporter.update_gross_exposure(abs(total_value))
            except Exception:
                pass

            # Exchange connectivity (simulate)
            self.exporter.update_exchange_connectivity("binance", True)

            # Default drawdown (would be calculated from actual trading history)
            self.exporter.update_max_drawdown(2.1, "daily")

        except Exception as exc:
            logger.error("System metrics collection failed: %s", exc)

    def collect_api_metrics(self, endpoint: str, method: str, duration_ms: float) -> None:
        """Collect API performance metrics"""
        self.exporter.update_api_latency(endpoint, method, duration_ms)

        # Cache for trend analysis
        self.metrics_cache[f"api_{endpoint}_{method}"].append(
            {"timestamp": time.time(), "duration_ms": duration_ms}
        )

    def collect_trading_metrics(
        self, symbol: str, side: str, status: str, duration_ms: Optional[float] = None
    ) -> None:
        """Collect trading-specific metrics"""
        self.exporter.increment_orders_executed(symbol, side, status)

        if duration_ms is not None:
            self.exporter.update_trading_loop_time(duration_ms)

    def get_cached_metrics(self, metric_name: str, minutes: int = 5) -> List[Dict[str, Any]]:
        """Get cached metrics for trend analysis"""
        cutoff_time = time.time() - (minutes * 60)
        cached = self.metrics_cache.get(metric_name, [])
        return [m for m in cached if m["timestamp"] > cutoff_time]


# Global instances
prometheus_exporter: Optional[PrometheusExporter] = None
metrics_collector: Optional[MetricsCollector] = None


def start_metrics_server(port: int = 9090) -> PrometheusExporter:
    """Start Prometheus metrics server"""
    global prometheus_exporter, metrics_collector

    if prometheus_exporter is None:
        prometheus_exporter = PrometheusExporter(port)
        metrics_collector = MetricsCollector(prometheus_exporter)

    return prometheus_exporter


async def initialize_monitoring(port: int = 9090) -> None:
    """Initialize complete monitoring system"""
    global prometheus_exporter, metrics_collector

    # Start Prometheus server
    prometheus_exporter = start_metrics_server(port)
    await prometheus_exporter.start_server()

    # Start metrics collection
    if metrics_collector:
        await metrics_collector.start_collection()

    logger.info("ðŸš€ Supreme System V5 monitoring fully initialized")


def get_metrics_summary() -> Dict[str, Any]:
    """Get comprehensive metrics summary"""
    if not prometheus_exporter:
        return {"status": "not_initialized"}

    summary = prometheus_exporter.get_metrics_summary()

    if metrics_collector:
        summary["collector"] = {
            "collecting": metrics_collector.is_collecting,
            "interval_seconds": metrics_collector.collection_interval,
            "cached_metrics": len(metrics_collector.metrics_cache),
        }

    return summary


# Convenience function for API middleware

def track_api_request(endpoint: str, method: str, duration_ms: float) -> None:
    """Track API request metrics"""
    if metrics_collector:
        metrics_collector.collect_api_metrics(endpoint, method, duration_ms)


# Convenience function for trading metrics

def track_trading_activity(
    symbol: str, side: str, status: str, duration_ms: Optional[float] = None
) -> None:
    """Track trading activity metrics"""
    if metrics_collector:
        metrics_collector.collect_trading_metrics(symbol, side, status, duration_ms)


if __name__ == "__main__":
    # Demo monitoring setup
    async def demo() -> None:
        print("ðŸ“ˆ Supreme System V5 Monitoring Demo")
        print("=" * 40)

        # Initialize monitoring
        await initialize_monitoring(9091)  # Use different port for demo

        # Simulate some metrics
        if prometheus_exporter:
            prometheus_exporter.update_websocket_clients(5)
            prometheus_exporter.update_api_latency("/api/v1/status", "GET", 15.2)
            prometheus_exporter.update_pnl_daily(125.50)
            prometheus_exporter.update_exchange_connectivity("binance", True)

        # Show summary
        summary = get_metrics_summary()
        print("ðŸ“‰ Metrics Summary:")
        print(f"   Prometheus available: {summary.get('prometheus_available')}")
        print(f"   Server running: {summary.get('server_running')}")
        print(f"   Core metrics: {len(summary.get('core_metrics', []))}")

        # Wait a bit then stop
        await asyncio.sleep(2)

        if prometheus_exporter:
            prometheus_exporter.stop_server()
        if metrics_collector:
            metrics_collector.stop_collection()

        print("ðŸš€ Monitoring demo completed")

    asyncio.run(demo())
