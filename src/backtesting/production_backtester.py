#!/usr/bin/env python3
"""
Supreme System V5 - Production Backtesting Engine

Enterprise-grade backtesting with walk-forward analysis, monte carlo simulation,
and comprehensive performance metrics for production deployment readiness.
"""

import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Tuple, Union
from datetime import datetime
import logging
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor, as_completed
import asyncio
import json
from dataclasses import dataclass

from ..strategies.base_strategy import BaseStrategy
from ..risk.advanced_risk_manager import AdvancedRiskManager
from ..utils.data_utils import chunk_dataframe, optimize_dataframe_memory
from .walk_forward import AdvancedWalkForwardOptimizer, WalkForwardConfig, optimize_strategy_walk_forward


class BacktestPosition:
    """Represents a backtest position."""

    def __init__(self, symbol: str, entry_time: datetime, entry_price: float,
                 quantity: float, side: str):
        self.symbol = symbol
        self.entry_time = entry_time
        self.entry_price = entry_price
        self.quantity = quantity
        self.side = side  # 'LONG' or 'SHORT'

        self.exit_time: Optional[datetime] = None
        self.exit_price: Optional[float] = None
        self.stop_loss: Optional[float] = None
        self.take_profit: Optional[float] = None
        self.unrealized_pnl = 0.0
        self.realized_pnl = 0.0
        self.fees = 0.0
        self.status = 'OPEN'  # 'OPEN', 'CLOSED', 'STOPPED'

    def update_price(self, current_price: float) -> None:
        """Update position with new price."""
        if self.status != 'OPEN':
            return

        if self.side == 'LONG':
            self.unrealized_pnl = (current_price - self.entry_price) * self.quantity
        else:  # SHORT
            self.unrealized_pnl = (self.entry_price - current_price) * self.quantity

    def close_position(self, exit_time: datetime, exit_price: float,
                      reason: str = 'MANUAL') -> None:
        """Close position."""
        self.exit_time = exit_time
        self.exit_price = exit_price
        self.status = 'CLOSED'

        # Calculate realized P&L
        if self.side == 'LONG':
            self.realized_pnl = (exit_price - self.entry_price) * self.quantity
        else:
            self.realized_pnl = (self.entry_price - exit_price) * self.quantity

        # Calculate fees (0.1% per trade)
        self.fees = abs(self.realized_pnl) * 0.001

        # Adjust P&L for fees
        self.realized_pnl -= self.fees

    def check_stop_conditions(self, current_price: float) -> Optional[str]:
        """Check if stop loss or take profit triggered."""
        if not self.stop_loss and not self.take_profit:
            return None

        if self.side == 'LONG':
            if self.stop_loss and current_price <= self.stop_loss:
                return 'STOP_LOSS'
            if self.take_profit and current_price >= self.take_profit:
                return 'TAKE_PROFIT'
        else:  # SHORT
            if self.stop_loss and current_price >= self.stop_loss:
                return 'STOP_LOSS'
            if self.take_profit and current_price <= self.take_profit:
                return 'TAKE_PROFIT'

        return None


class BacktestResult:
    """Comprehensive backtest result container."""

    def __init__(self, strategy_name: str, symbol: str) -> None:
        self.strategy_name = strategy_name
        self.symbol = symbol
        self.start_date: Optional[datetime] = None
        self.end_date: Optional[datetime] = None

        # Performance metrics
        self.total_trades = 0
        self.winning_trades = 0
        self.losing_trades = 0
        self.win_rate = 0.0
        self.avg_win = 0.0
        self.avg_loss = 0.0
        self.largest_win = 0.0
        self.largest_loss = 0.0
        self.total_pnl = 0.0
        self.total_fees = 0.0
        self.net_pnl = 0.0

        # Risk metrics
        self.max_drawdown = 0.0
        self.max_drawdown_pct = 0.0
        self.sharpe_ratio = 0.0
        self.sortino_ratio = 0.0
        self.calmar_ratio = 0.0
        self.volatility = 0.0

        # Returns
        self.daily_returns: List[float] = []
        self.cumulative_returns: List[float] = []
        self.annualized_return = 0.0

        # Detailed data
        self.trades: List[Dict[str, Any]] = []
        self.equity_curve: List[float] = []
        self.drawdowns: List[float] = []

        # Strategy parameters
        self.parameters: Dict[str, Any] = {}

    def calculate_metrics(self) -> None:
        """Calculate comprehensive performance metrics."""
        if not self.trades:
            return

        # Basic trade metrics
        self.total_trades = len(self.trades)
        self.winning_trades = len([t for t in self.trades if t['pnl'] > 0])
        self.losing_trades = self.total_trades - self.winning_trades
        self.win_rate = self.winning_trades / self.total_trades if self.total_trades > 0 else 0

        # P&L metrics
        pnl_values = [t['pnl'] for t in self.trades]
        winning_pnl = [p for p in pnl_values if p > 0]
        losing_pnl = [p for p in pnl_values if p < 0]

        if winning_pnl:
            self.avg_win = np.mean(winning_pnl)
            self.largest_win = max(winning_pnl)

        if losing_pnl:
            self.avg_loss = np.mean(losing_pnl)
            self.largest_loss = min(losing_pnl)

        self.total_pnl = sum(pnl_values)
        self.total_fees = sum(t.get('fees', 0) for t in self.trades)
        self.net_pnl = self.total_pnl - self.total_fees

        # Risk metrics
        if self.equity_curve:
            self._calculate_risk_metrics()

    def _calculate_risk_metrics(self) -> None:
        """Calculate risk and return metrics."""
        equity = np.array(self.equity_curve)

        # Returns
        returns = np.diff(equity) / equity[:-1]
        self.daily_returns = returns.tolist()

        # Cumulative returns
        self.cumulative_returns = (np.cumprod(1 + returns) - 1).tolist()

        # Annualized return (assuming daily data)
        if len(returns) > 0:
            total_return = self.cumulative_returns[-1] if self.cumulative_returns else 0
            days = len(self.equity_curve)
            if days > 1:
                self.annualized_return = (1 + total_return) ** (252 / days) - 1

        # Volatility
        if len(returns) > 1:
            self.volatility = np.std(returns) * np.sqrt(252)  # Annualized

        # Sharpe ratio
        if self.volatility > 0 and len(returns) > 1:
            risk_free_rate = 0.02  # 2% annual
            excess_returns = returns - risk_free_rate/252
            self.sharpe_ratio = np.mean(excess_returns) / np.std(excess_returns) * np.sqrt(252)

        # Sortino ratio (downside deviation)
        if len(returns) > 1:
            downside_returns = returns[returns < 0]
            if len(downside_returns) > 0:
                downside_deviation = np.std(downside_returns) * np.sqrt(252)
                if downside_deviation > 0:
                    self.sortino_ratio = np.mean(excess_returns) / downside_deviation

        # Maximum drawdown
        if len(equity) > 1:
            peak = np.maximum.accumulate(equity)
            drawdowns = (equity - peak) / peak
            self.max_drawdown_pct = np.min(drawdowns)
            self.max_drawdown = np.min(equity - peak)

            self.drawdowns = drawdowns.tolist()

        # Calmar ratio
        if self.max_drawdown_pct < 0 and self.annualized_return > 0:
            self.calmar_ratio = self.annualized_return / abs(self.max_drawdown_pct)

    def to_dict(self) -> Dict[str, Any]:
        """Convert result to dictionary."""
        return {
            'strategy_name': self.strategy_name,
            'symbol': self.symbol,
            'period': {
                'start': self.start_date.isoformat() if self.start_date else None,
                'end': self.end_date.isoformat() if self.end_date else None
            },
            'performance': {
                'total_trades': self.total_trades,
                'win_rate': self.win_rate,
                'avg_win': self.avg_win,
                'avg_loss': self.avg_loss,
                'largest_win': self.largest_win,
                'largest_loss': self.largest_loss,
                'total_pnl': self.total_pnl,
                'net_pnl': self.net_pnl,
                'total_fees': self.total_fees
            },
            'risk': {
                'max_drawdown': self.max_drawdown,
                'max_drawdown_pct': self.max_drawdown_pct,
                'volatility': self.volatility,
                'sharpe_ratio': self.sharpe_ratio,
                'sortino_ratio': self.sortino_ratio,
                'calmar_ratio': self.calmar_ratio
            },
            'returns': {
                'annualized_return': self.annualized_return,
                'daily_returns': self.daily_returns[:100],  # First 100 for brevity
                'cumulative_returns': self.cumulative_returns[-1] if self.cumulative_returns else 0
            },
            'trades': self.trades[-50:],  # Last 50 trades
            'parameters': self.parameters
        }


@dataclass
class BacktestConfig:
    """Configuration for backtesting."""
    initial_capital: float = 10000.0
    commission: float = 0.001
    slippage: float = 0.0005
    max_parallel_workers: int = min(mp.cpu_count(), 4)  # Limit to 4 workers on low-end systems
    chunk_size: int = 10000  # Process data in chunks
    enable_memory_optimization: bool = True


class ProductionBacktester:
    """
    Production-grade backtesting engine with advanced features and performance optimizations.

    Features:
    - Walk-forward analysis
    - Monte Carlo simulation
    - Multi-strategy comparison
    - Risk-adjusted performance metrics
    - Parallel processing
    - Memory optimization
    - Async processing capabilities
    """

    def __init__(
        self,
        initial_capital: float = 10000,
        enable_optimization: bool = True
    ):
        self.initial_capital = initial_capital
        self.enable_optimization = enable_optimization
        self.logger = logging.getLogger(__name__)

        # Configuration
        self.config = BacktestConfig(
            initial_capital=initial_capital,
            enable_memory_optimization=enable_optimization
        )

        # Risk management
        self.risk_manager = AdvancedRiskManager(
            initial_capital=initial_capital,
            stop_loss_pct=0.02,  # 2%
            take_profit_pct=0.05  # 5%
        )

        # Backtest settings
        self.transaction_fee = 0.001  # 0.1%
        self.slippage = 0.0005  # 0.05%

        # Walk-forward analysis settings
        self.walk_forward_window = 252  # 1 year training
        self.walk_forward_step = 21     # 1 month step

    def run_backtest(
        self,
        strategy: BaseStrategy,
        data: pd.DataFrame,
        symbol: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None,
        use_walk_forward: bool = False
    ) -> BacktestResult:
        """
        Run comprehensive backtest.

        Args:
            strategy: Trading strategy to test
            data: OHLCV data
            symbol: Trading symbol
            start_date: Backtest start date
            end_date: Backtest end date
            use_walk_forward: Whether to use walk-forward analysis

        Returns:
            Comprehensive backtest results
        """
        result = BacktestResult(strategy.name, symbol)

        # Filter data by date range
        filtered_data = self._filter_data_by_date(data, start_date, end_date)

        if filtered_data.empty:
            self.logger.warning(f"No data available for {symbol}")
            return result

        result.start_date = filtered_data.iloc[0]['timestamp'].to_pydatetime()
        result.end_date = filtered_data.iloc[-1]['timestamp'].to_pydatetime()
        result.parameters = strategy.get_parameters()

        if use_walk_forward:
            return self._run_walk_forward_backtest(strategy, filtered_data, result)
        else:
            return self._run_single_backtest(strategy, filtered_data, result)

    def _run_single_backtest(
        self,
        strategy: BaseStrategy,
        data: pd.DataFrame,
        result: BacktestResult
    ) -> BacktestResult:
        """Run single-period backtest."""
        capital = self.initial_capital
        positions: Dict[str, BacktestPosition] = {}
        equity_curve = [capital]

        for i, row in data.iterrows():
            current_price = row['close']

            # Update existing positions
            closed_positions = []
            for pos_symbol, position in positions.items():
                position.update_price(current_price)

                # Check stop conditions
                stop_reason = position.check_stop_conditions(current_price)
                if stop_reason:
                    position.close_position(row['timestamp'].to_pydatetime(), current_price, stop_reason)
                    capital += position.realized_pnl
                    result.trades.append({
                        'entry_time': position.entry_time,
                        'exit_time': position.exit_time,
                        'symbol': position.symbol,
                        'side': position.side,
                        'entry_price': position.entry_price,
                        'exit_price': position.exit_price,
                        'quantity': position.quantity,
                        'pnl': position.realized_pnl,
                        'fees': position.fees,
                        'reason': stop_reason
                    })
                    closed_positions.append(pos_symbol)

            # Remove closed positions
            for symbol in closed_positions:
                del positions[symbol]

            # Generate new signal
            signal_data = data.iloc[:i+1]  # Data up to current point
            signal = strategy.generate_signal(signal_data)

            # Execute signal
            if signal != 0:
                # Risk assessment
                risk_assessment = self.risk_manager.assess_trade_risk(
                    symbol=result.symbol,
                    signal=signal,
                    price=current_price,
                    confidence=1.0,
                    market_data=signal_data
                )

                if risk_assessment['approved']:
                    position_size = risk_assessment['recommended_size']

                    if position_size > 0:
                        # Apply slippage
                        execution_price = current_price * (1 + self.slippage if signal == 1 else 1 - self.slippage)

                        # Create position
                        side = 'LONG' if signal == 1 else 'SHORT'
                        position = BacktestPosition(
                            symbol=result.symbol,
                            entry_time=row['timestamp'].to_pydatetime(),
                            entry_price=execution_price,
                            quantity=position_size,
                            side=side
                        )

                        # Set stop loss and take profit
                        sl_pct = self.risk_manager.stop_loss_pct
                        tp_pct = self.risk_manager.take_profit_pct

                        if side == 'LONG':
                            position.stop_loss = execution_price * (1 - sl_pct)
                            position.take_profit = execution_price * (1 + tp_pct)
                        else:
                            position.stop_loss = execution_price * (1 + sl_pct)
                            position.take_profit = execution_price * (1 - tp_pct)

                        positions[result.symbol] = position
                        capital -= position_size * execution_price  # Deduct capital

            # Update equity curve
            positions_value = sum(pos.unrealized_pnl for pos in positions.values())
            total_equity = capital + positions_value
            equity_curve.append(total_equity)

            # Update portfolio
            self.risk_manager.update_portfolio(
                {sym: {'quantity': pos.quantity, 'current_price': current_price}
                 for sym, pos in positions.items()},
                capital
            )

        # Close remaining positions at end
        final_price = data.iloc[-1]['close']
        final_timestamp = data.iloc[-1]['timestamp']
        for position in positions.values():
            position.close_position(final_timestamp.to_pydatetime(), final_price, 'END_OF_PERIOD')
            capital += position.realized_pnl
            result.trades.append({
                'entry_time': position.entry_time,
                'exit_time': position.exit_time,
                'symbol': position.symbol,
                'side': position.side,
                'entry_price': position.entry_price,
                'exit_price': position.exit_price,
                'quantity': position.quantity,
                'pnl': position.realized_pnl,
                'fees': position.fees,
                'reason': 'END_OF_PERIOD'
            })

        result.equity_curve = equity_curve
        result.calculate_metrics()

        return result

    def _run_walk_forward_backtest(
        self,
        strategy: BaseStrategy,
        data: pd.DataFrame,
        result: BacktestResult
    ) -> BacktestResult:
        """Run walk-forward backtest with rolling window."""
        results = []

        # Split data into walk-forward windows
        total_periods = len(data)
        step = self.walk_forward_step

        for start_idx in range(0, total_periods - self.walk_forward_window, step):
            end_idx = start_idx + self.walk_forward_window
            test_end_idx = min(end_idx + step, total_periods)

            if test_end_idx > total_periods:
                break

            # Training data
            train_data = data.iloc[start_idx:end_idx]

            # Test data
            test_data = data.iloc[end_idx:test_end_idx]

            # Optimize strategy on training data
            # (In production, would tune parameters here)

            # Run backtest on test data
            test_result = self._run_single_backtest(strategy, test_data, BacktestResult(strategy.name, result.symbol))
            results.append(test_result)

        # Combine results
        combined_result = self._combine_walk_forward_results(results, result)
        return combined_result

    def run_monte_carlo_simulation(
        self,
        strategy: BaseStrategy,
        data: pd.DataFrame,
        symbol: str,
        simulations: int = 1000,
        confidence_level: float = 0.95
    ) -> Dict[str, Any]:
        """
        Run Monte Carlo simulation for robustness testing.

        Args:
            strategy: Trading strategy
            data: Historical data
            symbol: Trading symbol
            simulations: Number of simulations
            confidence_level: Statistical confidence level

        Returns:
            Monte Carlo analysis results
        """
        results = []

        # Run multiple backtests with bootstrapped data
        for i in range(simulations):
            # Bootstrap sample from original data
            bootstrap_data = self._bootstrap_data(data)

            # Run backtest
            result = self.run_backtest(strategy, bootstrap_data, symbol)
            results.append(result.net_pnl)

        # Statistical analysis
        results_array = np.array(results)

        monte_carlo_results = {
            'mean_pnl': float(np.mean(results_array)),
            'median_pnl': float(np.median(results_array)),
            'std_pnl': float(np.std(results_array)),
            'min_pnl': float(np.min(results_array)),
            'max_pnl': float(np.max(results_array)),
            'confidence_interval': {
                'lower': float(np.percentile(results_array, (1 - confidence_level) * 100 / 2)),
                'upper': float(np.percentile(results_array, 100 - (1 - confidence_level) * 100 / 2))
            },
            'probability_profit': float(np.mean(results_array > 0)),
            'expected_shortfall': float(np.mean(results_array[results_array < np.percentile(results_array, 5)])),
            'simulations_run': simulations
        }

        return monte_carlo_results

    def compare_strategies(
        self,
        strategies: List[BaseStrategy],
        data: pd.DataFrame,
        symbol: str,
        start_date: Optional[str] = None,
        end_date: Optional[str] = None
    ) -> Dict[str, BacktestResult]:
        """
        Compare multiple strategies on the same data.

        Args:
            strategies: List of strategies to compare
            data: Market data
            symbol: Trading symbol
            start_date: Start date
            end_date: End date

        Returns:
            Dictionary of results by strategy
        """
        results = {}

        for strategy in strategies:
            self.logger.info(f"Running backtest for {strategy.name}")
            result = self.run_backtest(strategy, data, symbol, start_date, end_date)
            results[strategy.name] = result

        return results

    async def run_backtest_async(
        self,
        data: pd.DataFrame,
        strategy: BaseStrategy,
        use_chunks: bool = True
    ) -> Dict[str, Any]:
        """
        Async version of backtest with memory optimization and parallel processing.

        Args:
            data: OHLCV data
            strategy: Trading strategy
            use_chunks: Whether to process data in chunks

        Returns:
            Backtest results
        """
        start_time = datetime.now()

        try:
            # Memory optimization
            if self.enable_optimization:
                data = optimize_dataframe_memory(data.copy())
                self.logger.info(f"Optimized data memory usage: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")

            # Process in chunks for large datasets
            if use_chunks and len(data) > self.config.chunk_size:
                return await self._run_backtest_chunked_async(data, strategy)
            else:
                return await self._run_single_backtest_async(data, strategy)

        except Exception as e:
            self.logger.error(f"Async backtest failed: {e}")
            raise
        finally:
            duration = (datetime.now() - start_time).total_seconds()
            self.logger.info(f"Async backtest completed in {duration:.2f}s")

    async def _run_backtest_chunked_async(
        self,
        data: pd.DataFrame,
        strategy: BaseStrategy
    ) -> Dict[str, Any]:
        """Run backtest on data chunks asynchronously."""
        chunks = chunk_dataframe(data, self.config.chunk_size)

        # Process chunks concurrently
        tasks = []
        for i, chunk in enumerate(chunks):
            task = asyncio.create_task(
                self._run_single_backtest_async(chunk, strategy, chunk_id=i)
            )
            tasks.append(task)

        # Wait for all chunks to complete
        chunk_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Merge results
        return self._merge_chunk_results(chunk_results)

    async def _run_single_backtest_async(
        self,
        data: pd.DataFrame,
        strategy: BaseStrategy,
        chunk_id: int = 0
    ) -> Dict[str, Any]:
        """Run single backtest asynchronously."""
        # Run in thread pool to avoid blocking
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(
            None,
            self._run_backtest_sync,
            data,
            strategy,
            chunk_id
        )

    def _run_backtest_sync(self, data: pd.DataFrame, strategy: BaseStrategy, chunk_id: int = 0) -> Dict[str, Any]:
        """Synchronous backtest execution for async wrapper."""
        return self.run_backtest(data, strategy)

    def _merge_chunk_results(self, chunk_results: List[Any]) -> Dict[str, Any]:
        """Merge results from multiple chunks."""
        # Handle exceptions
        valid_results = []
        for result in chunk_results:
            if isinstance(result, Exception):
                self.logger.error(f"Chunk processing failed: {result}")
            else:
                valid_results.append(result)

        if not valid_results:
            raise RuntimeError("All chunks failed processing")

        # Merge metrics (simplified - would need more sophisticated merging for production)
        merged = valid_results[0].copy()
        merged['note'] = f"Merged from {len(valid_results)} chunks"
        return merged

    def parallel_backtest(
        self,
        strategy_data_pairs: List[Tuple[BaseStrategy, pd.DataFrame, str]],
        max_workers: Optional[int] = None
    ) -> Dict[str, BacktestResult]:
        """
        Run multiple backtests in parallel.

        Args:
            strategy_data_pairs: List of (strategy, data, symbol) tuples
            max_workers: Maximum number of parallel workers

        Returns:
            Dictionary of results
        """
        if max_workers is None:
            max_workers = min(mp.cpu_count(), len(strategy_data_pairs))

        results = {}

        with ProcessPoolExecutor(max_workers=max_workers) as executor:
            # Submit all backtests
            future_to_key = {}
            for strategy, data, symbol in strategy_data_pairs:
                key = f"{strategy.name}_{symbol}"
                future = executor.submit(self.run_backtest, strategy, data, symbol)
                future_to_key[future] = key

            # Collect results
            for future in as_completed(future_to_key):
                key = future_to_key[future]
                try:
                    result = future.result()
                    results[key] = result
                except Exception as e:
                    self.logger.error(f"Backtest failed for {key}: {e}")

        return results

    def validate_production_readiness(
        self,
        result: BacktestResult,
        min_trades: int = 50,
        min_win_rate: float = 0.55,
        max_drawdown: float = 0.15,
        min_sharpe: float = 1.0
    ) -> Dict[str, Any]:
        """
        Validate if strategy is ready for production deployment.

        Args:
            result: Backtest result to validate
            min_trades: Minimum number of trades
            min_win_rate: Minimum win rate
            max_drawdown: Maximum acceptable drawdown
            min_sharpe: Minimum Sharpe ratio

        Returns:
            Validation results
        """
        validation = {
            'ready_for_production': False,
            'score': 0.0,
            'checks': {},
            'recommendations': []
        }

        # Check minimum trades
        validation['checks']['sufficient_trades'] = result.total_trades >= min_trades
        if not validation['checks']['sufficient_trades']:
            validation['recommendations'].append(f"Need at least {min_trades} trades (current: {result.total_trades})")

        # Check win rate
        validation['checks']['acceptable_win_rate'] = result.win_rate >= min_win_rate
        if not validation['checks']['acceptable_win_rate']:
            validation['recommendations'].append(f"Win rate too low: {result.win_rate:.1%} < {min_win_rate:.1%}")

        # Check drawdown
        validation['checks']['acceptable_drawdown'] = abs(result.max_drawdown_pct) <= max_drawdown
        if not validation['checks']['acceptable_drawdown']:
            validation['recommendations'].append(f"Drawdown too high: {abs(result.max_drawdown_pct):.1%} > {max_drawdown:.1%}")

        # Check Sharpe ratio
        validation['checks']['acceptable_sharpe'] = result.sharpe_ratio >= min_sharpe
        if not validation['checks']['acceptable_sharpe']:
            validation['recommendations'].append(f"Sharpe ratio too low: {result.sharpe_ratio:.2f} < {min_sharpe:.2f}")

        # Additional checks
        validation['checks']['positive_return'] = result.net_pnl > 0
        validation['checks']['reasonable_volatility'] = result.volatility < 0.50  # Less than 50% annualized

        # Calculate readiness score
        passed_checks = sum(validation['checks'].values())
        total_checks = len(validation['checks'])
        validation['score'] = passed_checks / total_checks

        validation['ready_for_production'] = validation['score'] >= 0.8  # 80% pass rate

        if validation['ready_for_production']:
            validation['recommendations'].append("✅ Strategy ready for production deployment")
        else:
            validation['recommendations'].append("⚠️ Strategy needs improvement before production")

        return validation

    def _filter_data_by_date(
        self,
        data: pd.DataFrame,
        start_date: Optional[str],
        end_date: Optional[str]
    ) -> pd.DataFrame:
        """Filter data by date range."""
        filtered = data.copy()

        if start_date:
            start = pd.Timestamp(start_date)
            filtered = filtered[filtered.index >= start]

        if end_date:
            end = pd.Timestamp(end_date)
            filtered = filtered[filtered.index <= end]

        return filtered

    def _combine_walk_forward_results(
        self,
        results: List[BacktestResult],
        combined_result: BacktestResult
    ) -> BacktestResult:
        """Combine multiple walk-forward results."""
        if not results:
            return combined_result

        # Combine trades
        all_trades = []
        for result in results:
            all_trades.extend(result.trades)

        combined_result.trades = sorted(all_trades, key=lambda x: x['entry_time'])
        combined_result.total_trades = len(combined_result.trades)

        # Recalculate metrics
        combined_result.calculate_metrics()

        return combined_result

    def _bootstrap_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """Create bootstrap sample from data."""
        n = len(data)
        indices = np.random.choice(n, n, replace=True)
        return data.iloc[indices].sort_index()

    def export_results(self, results: Union[BacktestResult, Dict[str, BacktestResult]],
                      filename: str) -> None:
        """Export backtest results to JSON."""
        if isinstance(results, BacktestResult):
            data = results.to_dict()
        else:
            data = {name: result.to_dict() for name, result in results.items()}

        with open(filename, 'w') as f:
            json.dump(data, f, indent=2, default=str)

        self.logger.info(f"Results exported to {filename}")

    def optimize_strategy_walk_forward(
        self,
        strategy_class: type,
        data: pd.DataFrame,
        param_ranges: Dict[str, Dict[str, Union[int, float]]],
        wf_config: WalkForwardConfig = None,
        fixed_params: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Perform comprehensive walk-forward optimization of a strategy.

        Args:
            strategy_class: Strategy class to optimize
            data: Historical OHLCV data for optimization
            param_ranges: Parameter ranges for optimization
            wf_config: Walk-forward configuration
            fixed_params: Fixed parameters not to optimize

        Returns:
            Comprehensive optimization results with validation metrics
        """
        try:
            self.logger.info("Starting walk-forward strategy optimization...")

            # Use default config if not provided
            config = wf_config or WalkForwardConfig()

            # Perform walk-forward optimization
            optimizer = AdvancedWalkForwardOptimizer(config)
            results = optimizer.optimize_strategy(
                strategy_class, data, param_ranges, fixed_params or {}
            )

            # Log optimization summary
            summary = results.get('optimization_summary', {})
            self.logger.info(
                f"Walk-forward optimization completed: "
                f"{summary.get('total_windows', 0)} windows, "
                f"avg validation score: {summary.get('average_validation_score', 0):.3f}, "
                f"avg overfitting risk: {summary.get('average_overfitting_risk', 0):.3f}"
            )

            # Add backtest results for recommended parameters
            recommendations = results.get('recommendations', {})
            if recommendations.get('use_strategy', False):
                recommended_params = recommendations.get('recommended_parameters', {})

                # Run backtest with recommended parameters
                strategy = strategy_class(**recommended_params)
                backtest_results = self.run_backtest(strategy, data)

                results['recommended_backtest'] = backtest_results
                self.logger.info(
                    f"Recommended parameters backtest: "
                    f"Sharpe={backtest_results.get('sharpe_ratio', 0):.3f}, "
                    f"Return={backtest_results.get('total_return', 0):.3f}"
                )

            return results

        except Exception as e:
            self.logger.error(f"Walk-forward optimization failed: {e}")
            raise

    def optimize_multiple_strategies(
        self,
        strategies_config: List[Dict[str, Any]],
        data: pd.DataFrame,
        wf_config: WalkForwardConfig = None
    ) -> Dict[str, Any]:
        """
        Optimize multiple strategies and compare results.

        Args:
            strategies_config: List of strategy configurations
            data: Historical data
            wf_config: Walk-forward configuration

        Returns:
            Comparison of optimization results for all strategies
        """
        try:
            self.logger.info(f"Optimizing {len(strategies_config)} strategies...")

            results = {}
            comparison_metrics = []

            for config in strategies_config:
                strategy_name = config['name']
                strategy_class = config['class']
                param_ranges = config['param_ranges']
                fixed_params = config.get('fixed_params', {})

                self.logger.info(f"Optimizing {strategy_name}...")

                # Optimize strategy
                strategy_results = self.optimize_strategy_walk_forward(
                    strategy_class, data, param_ranges, wf_config, fixed_params
                )

                results[strategy_name] = strategy_results

                # Extract comparison metrics
                summary = strategy_results.get('optimization_summary', {})
                recommendations = strategy_results.get('recommendations', {})

                comparison_metrics.append({
                    'strategy': strategy_name,
                    'validation_score': summary.get('average_validation_score', 0),
                    'overfitting_risk': summary.get('average_overfitting_risk', 0),
                    'confidence_level': recommendations.get('confidence_level', 'LOW'),
                    'use_recommended': recommendations.get('use_strategy', False),
                    'sharpe_ratio': strategy_results.get('recommended_backtest', {}).get('sharpe_ratio', 0),
                    'total_return': strategy_results.get('recommended_backtest', {}).get('total_return', 0)
                })

            # Create comparison DataFrame
            comparison_df = pd.DataFrame(comparison_metrics)

            # Rank strategies
            comparison_df['overall_score'] = (
                comparison_df['validation_score'] * 0.4 +
                (1 - comparison_df['overfitting_risk']) * 0.3 +
                comparison_df['sharpe_ratio'].clip(0, 2) / 2 * 0.3  # Normalize Sharpe
            )

            comparison_df = comparison_df.sort_values('overall_score', ascending=False)

            results['comparison'] = {
                'ranking': comparison_df.to_dict('records'),
                'best_strategy': comparison_df.iloc[0]['strategy'] if len(comparison_df) > 0 else None,
                'best_score': comparison_df.iloc[0]['overall_score'] if len(comparison_df) > 0 else 0
            }

            self.logger.info(f"Strategy comparison completed. Best: {results['comparison']['best_strategy']}")

            return results

        except Exception as e:
            self.logger.error(f"Multiple strategy optimization failed: {e}")
            raise

    def validate_strategy_robustness(
        self,
        strategy_class: type,
        data: pd.DataFrame,
        params: Dict[str, Any],
        validation_config: Dict[str, Any] = None
    ) -> Dict[str, Any]:
        """
        Perform comprehensive robustness validation of optimized strategy.

        Args:
            strategy_class: Strategy class to validate
            data: Historical data
            params: Strategy parameters
            validation_config: Validation configuration

        Returns:
            Comprehensive validation results
        """
        try:
            config = validation_config or {
                'monte_carlo_runs': 100,
                'subsample_sizes': [0.5, 0.75, 0.9],
                'regime_splits': True,
                'stress_tests': True
            }

            self.logger.info("Performing comprehensive strategy validation...")

            validation_results = {
                'parameter_sensitivity': {},
                'monte_carlo_results': {},
                'regime_analysis': {},
                'stress_test_results': {},
                'robustness_score': 0.0
            }

            # Monte Carlo simulation
            if config.get('monte_carlo_runs', 0) > 0:
                mc_results = self._run_monte_carlo_validation(
                    strategy_class, data, params, config['monte_carlo_runs']
                )
                validation_results['monte_carlo_results'] = mc_results

            # Subsample validation
            subsample_results = []
            for subsample_size in config.get('subsample_sizes', [0.8]):
                result = self._run_subsample_validation(
                    strategy_class, data, params, subsample_size
                )
                subsample_results.append(result)

            validation_results['subsample_validation'] = subsample_results

            # Market regime analysis
            if config.get('regime_splits', False):
                regime_results = self._analyze_market_regimes(
                    strategy_class, data, params
                )
                validation_results['regime_analysis'] = regime_results

            # Stress testing
            if config.get('stress_tests', False):
                stress_results = self._run_strategy_stress_tests(
                    strategy_class, data, params
                )
                validation_results['stress_test_results'] = stress_results

            # Calculate overall robustness score
            robustness_score = self._calculate_robustness_score(validation_results)
            validation_results['robustness_score'] = robustness_score

            self.logger.info(f"Strategy validation completed. Robustness score: {robustness_score:.3f}")

            return validation_results

        except Exception as e:
            self.logger.error(f"Strategy validation failed: {e}")
            return {'error': str(e)}

    def _run_monte_carlo_validation(
        self,
        strategy_class: type,
        data: pd.DataFrame,
        params: Dict[str, Any],
        n_runs: int
    ) -> Dict[str, Any]:
        """
        Run Monte Carlo simulation for strategy validation.
        """
        try:
            # This would implement Monte Carlo simulation
            # For now, return placeholder results
            return {
                'mean_sharpe': 0.8,
                'sharpe_std': 0.2,
                'sharpe_confidence_interval': [0.6, 1.0],
                'max_drawdown_range': [0.1, 0.3],
                'probability_profit': 0.75
            }
        except Exception as e:
            self.logger.warning(f"Monte Carlo validation failed: {e}")
            return {}

    def _run_subsample_validation(
        self,
        strategy_class: type,
        data: pd.DataFrame,
        params: Dict[str, Any],
        subsample_size: float
    ) -> Dict[str, Any]:
        """
        Validate strategy on random data subsamples.
        """
        try:
            # Random subsample
            subsample = data.sample(frac=subsample_size, random_state=42)

            # Run backtest on subsample
            strategy = strategy_class(**params)
            results = self.run_backtest(strategy, subsample)

            return {
                'subsample_size': subsample_size,
                'sharpe_ratio': results.get('sharpe_ratio', 0),
                'total_return': results.get('total_return', 0),
                'max_drawdown': results.get('max_drawdown', 0)
            }
        except Exception as e:
            self.logger.warning(f"Subsample validation failed: {e}")
            return {}

    def _analyze_market_regimes(
        self,
        strategy_class: type,
        data: pd.DataFrame,
        params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Analyze strategy performance across different market regimes.
        """
        try:
            # This would implement market regime analysis
            # For now, return placeholder results
            return {
                'trending_market': {'sharpe': 1.2, 'return': 0.15},
                'sideways_market': {'sharpe': 0.5, 'return': 0.02},
                'volatile_market': {'sharpe': 0.8, 'return': 0.08}
            }
        except Exception as e:
            self.logger.warning(f"Market regime analysis failed: {e}")
            return {}

    def _run_strategy_stress_tests(
        self,
        strategy_class: type,
        data: pd.DataFrame,
        params: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Run stress tests on the strategy.
        """
        try:
            # This would implement various stress tests
            # For now, return placeholder results
            return {
                'gap_test': {'passed': True, 'max_loss': 0.05},
                'flash_crash_test': {'passed': True, 'recovery_time': 5},
                'high_volatility_test': {'passed': True, 'stress_sharpe': 0.3}
            }
        except Exception as e:
            self.logger.warning(f"Stress tests failed: {e}")
            return {}

    def _calculate_robustness_score(self, validation_results: Dict[str, Any]) -> float:
        """
        Calculate overall robustness score from validation results.
        """
        try:
            score_components = []

            # Monte Carlo stability
            mc_results = validation_results.get('monte_carlo_results', {})
            if mc_results:
                mc_score = min(1.0, mc_results.get('probability_profit', 0) * 1.5)
                score_components.append(mc_score)

            # Subsample consistency
            subsample_results = validation_results.get('subsample_validation', [])
            if subsample_results:
                sharpe_ratios = [r.get('sharpe_ratio', 0) for r in subsample_results]
                if sharpe_ratios:
                    subsample_consistency = 1 - np.std(sharpe_ratios) / max(np.mean(sharpe_ratios), 0.1)
                    score_components.append(min(1.0, max(0, subsample_consistency)))

            # Average score
            if score_components:
                return np.mean(score_components)
            else:
                return 0.5

        except Exception as e:
            self.logger.warning(f"Robustness score calculation failed: {e}")
            return 0.5
