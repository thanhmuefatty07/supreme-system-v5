#!/usr/bin/env python3
"""
‚ö° Agentic Performance Benchmark Suite - Trading System Performance Validation
Comprehensive benchmarking with resource monitoring and performance regression detection

Usage:
    python scripts/agents/run_bench_suite.py
    python scripts/agents/run_bench_suite.py --mode enhanced
    python scripts/agents/run_bench_suite.py --component strategies --duration 60
    python scripts/agents/run_bench_suite.py --regression-test baseline.json
"""

import argparse
import asyncio
import json
import os
import psutil
import sys
import time
import tracemalloc
from collections import defaultdict, deque
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import statistics
import gc

project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "python"))

print("‚ö° Agentic Performance Benchmark Suite")
print("=" * 36)

class PerformanceBenchmarkSuite:
    """Comprehensive performance benchmarking for trading systems"""
    
    def __init__(self, mode: str = 'standard', output_dir: str = 'run_artifacts/benchmarks'):
        self.mode = mode
        self.project_root = project_root  
        self.python_root = self.project_root / "python"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        self.benchmark_results = {
            'timestamp': datetime.utcnow().isoformat(),
            'mode': mode,
            'system_info': self._collect_system_info(),
            'benchmarks': {},
            'resource_monitoring': {
                'memory_snapshots': [],
                'cpu_snapshots': [],
                'gc_stats': []
            },
            'performance_summary': {},
            'regression_analysis': None
        }
        
        # Performance tracking
        self.start_time = time.perf_counter()
        self.memory_tracker = deque(maxlen=1000)
        self.cpu_tracker = deque(maxlen=1000)
        
        # Enable memory tracking
        tracemalloc.start()
        
    def _collect_system_info(self) -> Dict[str, Any]:
        """Collect system information for benchmark context"""
        return {
            'platform': os.name,
            'python_version': sys.version,
            'cpu_count': psutil.cpu_count(),
            'cpu_freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
            'memory_total_mb': psutil.virtual_memory().total / (1024 * 1024),
            'memory_available_mb': psutil.virtual_memory().available / (1024 * 1024),
            'disk_usage': psutil.disk_usage('.')._asdict(),
            'load_average': os.getloadavg() if hasattr(os, 'getloadavg') else None
        }
        
    def start_resource_monitoring(self):
        """Start continuous resource monitoring"""
        self._monitoring_active = True
        
        async def monitor_loop():
            while self._monitoring_active:
                try:
                    # Memory snapshot
                    memory_info = psutil.Process().memory_info()
                    memory_snapshot = {
                        'timestamp': time.perf_counter() - self.start_time,
                        'rss_mb': memory_info.rss / (1024 * 1024),
                        'vms_mb': memory_info.vms / (1024 * 1024),
                        'memory_percent': psutil.Process().memory_percent()
                    }
                    
                    # Add tracemalloc info
                    current, peak = tracemalloc.get_traced_memory()
                    memory_snapshot.update({
                        'traced_current_mb': current / (1024 * 1024),
                        'traced_peak_mb': peak / (1024 * 1024)
                    })
                    
                    self.benchmark_results['resource_monitoring']['memory_snapshots'].append(memory_snapshot)
                    self.memory_tracker.append(memory_snapshot['rss_mb'])
                    
                    # CPU snapshot
                    cpu_percent = psutil.cpu_percent(interval=0.1)
                    cpu_snapshot = {
                        'timestamp': time.perf_counter() - self.start_time,
                        'cpu_percent': cpu_percent,
                        'load_average_1m': os.getloadavg()[0] if hasattr(os, 'getloadavg') else None
                    }
                    
                    self.benchmark_results['resource_monitoring']['cpu_snapshots'].append(cpu_snapshot)
                    self.cpu_tracker.append(cpu_percent)
                    
                    # GC stats
                    gc_stats = {
                        'timestamp': time.perf_counter() - self.start_time,
                        'collections': gc.get_stats(),
                        'objects': len(gc.get_objects())
                    }
                    
                    self.benchmark_results['resource_monitoring']['gc_stats'].append(gc_stats)
                    
                    await asyncio.sleep(1.0)  # Monitor every second
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Resource monitoring error: {e}")
                    await asyncio.sleep(2.0)
                    
        # Start monitoring task
        self._monitor_task = asyncio.create_task(monitor_loop())
        
    def stop_resource_monitoring(self):
        """Stop resource monitoring"""
        self._monitoring_active = False
        if hasattr(self, '_monitor_task'):
            self._monitor_task.cancel()
            
    async def benchmark_indicators(self, iterations: int = 10000) -> Dict[str, Any]:
        """Benchmark technical indicators performance"""
        print(f"üìà Benchmarking indicators ({iterations:,} iterations)...")
        
        try:
            from supreme_system_v5.algorithms.ultra_optimized_indicators import (
                UltraOptimizedEMA, UltraOptimizedRSI, UltraOptimizedMACD
            )
            
            # Test data
            import random
            test_prices = [100.0 + random.gauss(0, 5) for _ in range(iterations)]
            
            benchmark_results = {}
            
            # Benchmark EMA
            ema = UltraOptimizedEMA(period=14)
            start_time = time.perf_counter()
            
            for price in test_prices:
                ema.update(price)
                
            ema_duration = time.perf_counter() - start_time
            ema_per_update_us = (ema_duration / iterations) * 1_000_000
            
            benchmark_results['ema'] = {
                'total_duration_s': ema_duration,
                'per_update_us': ema_per_update_us,
                'updates_per_second': iterations / ema_duration,
                'target_met': ema_per_update_us < 1.0  # <1Œºs target
            }
            
            print(f"   EMA: {ema_per_update_us:.2f}Œºs per update ({iterations/ema_duration:,.0f} ops/sec)")
            
            # Benchmark RSI
            rsi = UltraOptimizedRSI(period=14)
            start_time = time.perf_counter()
            
            for price in test_prices:
                rsi.update(price)
                
            rsi_duration = time.perf_counter() - start_time
            rsi_per_update_us = (rsi_duration / iterations) * 1_000_000
            
            benchmark_results['rsi'] = {
                'total_duration_s': rsi_duration,
                'per_update_us': rsi_per_update_us,
                'updates_per_second': iterations / rsi_duration,
                'target_met': rsi_per_update_us < 2.0  # <2Œºs target
            }
            
            print(f"   RSI: {rsi_per_update_us:.2f}Œºs per update ({iterations/rsi_duration:,.0f} ops/sec)")
            
            # Benchmark MACD
            macd = UltraOptimizedMACD()
            start_time = time.perf_counter()
            
            for price in test_prices:
                macd.update(price)
                
            macd_duration = time.perf_counter() - start_time
            macd_per_update_us = (macd_duration / iterations) * 1_000_000
            
            benchmark_results['macd'] = {
                'total_duration_s': macd_duration,
                'per_update_us': macd_per_update_us,
                'updates_per_second': iterations / macd_duration,
                'target_met': macd_per_update_us < 3.0  # <3Œºs target
            }
            
            print(f"   MACD: {macd_per_update_us:.2f}Œºs per update ({iterations/macd_duration:,.0f} ops/sec)")
            
            # Overall performance
            total_time = ema_duration + rsi_duration + macd_duration
            avg_per_update_us = (total_time / (3 * iterations)) * 1_000_000
            
            benchmark_results['overall'] = {
                'total_duration_s': total_time,
                'avg_per_update_us': avg_per_update_us,
                'combined_ops_per_second': (3 * iterations) / total_time,
                'all_targets_met': all(r['target_met'] for r in benchmark_results.values() if isinstance(r, dict) and 'target_met' in r)
            }
            
            print(f"   Overall: {avg_per_update_us:.2f}Œºs average per indicator update")
            
            return benchmark_results
            
        except ImportError as e:
            print(f"   ‚ö†Ô∏è Indicator import failed: {e}")
            return {'status': 'failed', 'error': 'import_error', 'message': str(e)}
        except Exception as e:
            print(f"   ‚ùå Indicator benchmark failed: {e}")
            return {'status': 'failed', 'error': 'benchmark_error', 'message': str(e)}
            
    async def benchmark_strategy(self, duration: int = 30) -> Dict[str, Any]:
        """Benchmark trading strategy performance"""
        print(f"üéØ Benchmarking strategy performance ({duration}s)...")
        
        try:
            from supreme_system_v5.strategies import ScalpingStrategy
            
            # Configure strategy
            strategy = ScalpingStrategy({
                'ema_period': 14,
                'rsi_period': 14,
                'position_size_pct': 0.02
            })
            
            # Performance tracking
            signal_times = []
            price_update_times = []
            signal_count = 0
            update_count = 0
            
            start_benchmark = time.perf_counter()
            end_time = start_benchmark + duration
            
            import random
            base_price = 3500.0
            
            while time.perf_counter() < end_time:
                # Generate realistic price
                price_change = random.gauss(0, 0.001)
                current_price = base_price * (1 + price_change)
                volume = random.uniform(800, 1200)
                
                # Benchmark price update
                update_start = time.perf_counter()
                
                try:
                    result = strategy.add_price_data(current_price, volume, time.time())
                    update_duration = time.perf_counter() - update_start
                    price_update_times.append(update_duration)
                    update_count += 1
                    
                    if result:
                        signal_count += 1
                        
                except Exception as e:
                    print(f"     Strategy error: {e}")
                    
                # Short sleep to prevent CPU spinning
                await asyncio.sleep(0.001)  # 1ms
                
            total_duration = time.perf_counter() - start_benchmark
            
            # Calculate metrics
            if price_update_times:
                avg_update_time_us = statistics.mean(price_update_times) * 1_000_000
                p95_update_time_us = statistics.quantiles(price_update_times, n=20)[18] * 1_000_000  # 95th percentile
                max_update_time_us = max(price_update_times) * 1_000_000
            else:
                avg_update_time_us = p95_update_time_us = max_update_time_us = 0
                
            benchmark_results = {
                'duration_s': total_duration,
                'updates_processed': update_count,
                'signals_generated': signal_count,
                'update_rate_per_second': update_count / total_duration,
                'signal_rate_per_second': signal_count / total_duration,
                'avg_update_time_us': avg_update_time_us,
                'p95_update_time_us': p95_update_time_us,
                'max_update_time_us': max_update_time_us,
                'target_performance': {
                    'latency_target_met': avg_update_time_us < 5000,  # <5ms
                    'throughput_target_met': (update_count / total_duration) > 100,  # >100 updates/sec
                    'signal_rate_adequate': (signal_count / total_duration) > 0.1  # >0.1 signals/sec
                }
            }
            
            # Performance status
            targets_met = sum(benchmark_results['target_performance'].values())
            benchmark_results['performance_grade'] = {
                3: 'EXCELLENT',
                2: 'GOOD', 
                1: 'ADEQUATE',
                0: 'NEEDS_IMPROVEMENT'
            }[targets_met]
            
            print(f"   Updates: {update_count:,} ({update_count/total_duration:.0f}/sec)")
            print(f"   Signals: {signal_count:,} ({signal_count/total_duration:.2f}/sec)")
            print(f"   Latency: {avg_update_time_us:.0f}Œºs avg, {p95_update_time_us:.0f}Œºs p95")
            print(f"   Grade: {benchmark_results['performance_grade']}")
            
            return benchmark_results
            
        except ImportError as e:
            print(f"   ‚ö†Ô∏è Strategy import failed: {e}")
            return {'status': 'failed', 'error': 'import_error', 'message': str(e)}
        except Exception as e:
            print(f"   ‚ùå Strategy benchmark failed: {e}")
            return {'status': 'failed', 'error': 'benchmark_error', 'message': str(e)}
            
    async def benchmark_data_fabric(self, duration: int = 20) -> Dict[str, Any]:
        """Benchmark data fabric performance"""
        print(f"üîå Benchmarking data fabric ({duration}s)...")
        
        try:
            # Mock data fabric benchmark
            start_benchmark = time.perf_counter()
            
            # Simulate data aggregation operations
            operations = ['fetch', 'normalize', 'validate', 'cache', 'aggregate']
            operation_times = {op: [] for op in operations}
            total_operations = 0
            
            end_time = start_benchmark + duration
            
            while time.perf_counter() < end_time:
                for operation in operations:
                    op_start = time.perf_counter()
                    
                    # Simulate operation (realistic workload)
                    if operation == 'fetch':
                        await asyncio.sleep(0.001)  # Network latency
                    elif operation == 'normalize':
                        # Simulate data transformation
                        data = [random.random() for _ in range(100)]
                        normalized = [(x - 0.5) * 2 for x in data]
                    elif operation == 'validate':
                        # Simulate validation
                        import random
                        validation_result = random.random() > 0.1  # 90% pass rate
                    elif operation == 'cache':
                        # Simulate cache operation
                        cache_key = f"data_{int(time.time() * 1000)}"
                    elif operation == 'aggregate':
                        # Simulate aggregation
                        values = [random.random() for _ in range(10)]
                        result = sum(values) / len(values)
                        
                    op_duration = time.perf_counter() - op_start
                    operation_times[operation].append(op_duration)
                    total_operations += 1
                    
                await asyncio.sleep(0.01)  # 10ms cycle
                
            total_duration = time.perf_counter() - start_benchmark
            
            # Calculate metrics
            benchmark_results = {
                'duration_s': total_duration,
                'total_operations': total_operations,
                'operations_per_second': total_operations / total_duration,
                'operation_breakdown': {}
            }
            
            for operation, times in operation_times.items():
                if times:
                    avg_time_us = statistics.mean(times) * 1_000_000
                    benchmark_results['operation_breakdown'][operation] = {
                        'count': len(times),
                        'avg_time_us': avg_time_us,
                        'total_time_s': sum(times),
                        'ops_per_second': len(times) / sum(times) if sum(times) > 0 else 0
                    }
                    
                    print(f"   {operation.capitalize()}: {avg_time_us:.0f}Œºs avg ({len(times)} ops)")
                    
            print(f"   Overall: {total_operations/total_duration:.0f} operations/sec")
            
            return benchmark_results
            
        except Exception as e:
            print(f"   ‚ùå Data fabric benchmark failed: {e}")
            return {'status': 'failed', 'error': str(e)}
            
    async def benchmark_risk_manager(self, iterations: int = 5000) -> Dict[str, Any]:
        """Benchmark risk management system"""
        print(f"üõ°Ô∏è Benchmarking risk manager ({iterations:,} iterations)...")
        
        try:
            from supreme_system_v5.risk import RiskManager, RiskLimits, PortfolioState
            
            # Setup risk manager
            limits = RiskLimits(
                max_drawdown_percent=10.0,
                max_daily_loss_usd=500.0,
                max_position_size_usd=1000.0,
                max_leverage=2.0
            )
            
            portfolio = PortfolioState(
                total_balance=10000.0,
                available_balance=10000.0
            )
            
            risk_manager = RiskManager(limits=limits, portfolio_state=portfolio)
            
            # Benchmark risk calculations
            calculation_times = []
            successful_evaluations = 0
            
            import random
            
            start_benchmark = time.perf_counter()
            
            for i in range(iterations):
                calc_start = time.perf_counter()
                
                try:
                    # Simulate trade evaluation
                    symbol = random.choice(['BTC-USDT', 'ETH-USDT'])
                    position_value = random.uniform(100, 1000)
                    leverage = random.uniform(1.0, 2.0)
                    
                    assessment = risk_manager.evaluate_trade(symbol, position_value, leverage)
                    
                    if assessment.approved or not assessment.approved:
                        successful_evaluations += 1
                        
                    calc_duration = time.perf_counter() - calc_start
                    calculation_times.append(calc_duration)
                    
                except Exception as e:
                    calc_duration = time.perf_counter() - calc_start
                    calculation_times.append(calc_duration)
                    
            total_duration = time.perf_counter() - start_benchmark
            
            # Calculate metrics
            avg_calc_time_us = statistics.mean(calculation_times) * 1_000_000
            p95_calc_time_us = statistics.quantiles(calculation_times, n=20)[18] * 1_000_000
            
            benchmark_results = {
                'duration_s': total_duration,
                'iterations': iterations,
                'successful_evaluations': successful_evaluations,
                'success_rate': successful_evaluations / iterations,
                'avg_calculation_time_us': avg_calc_time_us,
                'p95_calculation_time_us': p95_calc_time_us,
                'evaluations_per_second': iterations / total_duration,
                'target_performance': {
                    'latency_target_met': avg_calc_time_us < 100,  # <100Œºs
                    'throughput_target_met': (iterations / total_duration) > 1000,  # >1000/sec
                    'reliability_target_met': (successful_evaluations / iterations) > 0.95  # >95%
                }
            }
            
            print(f"   Calculations: {avg_calc_time_us:.0f}Œºs avg, {p95_calc_time_us:.0f}Œºs p95")
            print(f"   Throughput: {iterations/total_duration:.0f} evaluations/sec")
            print(f"   Success rate: {(successful_evaluations/iterations)*100:.1f}%")
            
            return benchmark_results
            
        except ImportError as e:
            print(f"   ‚ö†Ô∏è Risk manager import failed: {e}")
            return {'status': 'failed', 'error': 'import_error', 'message': str(e)}
        except Exception as e:
            print(f"   ‚ùå Risk manager benchmark failed: {e}")
            return {'status': 'failed', 'error': 'benchmark_error', 'message': str(e)}
            
    async def benchmark_backtest_engine(self, duration: int = 60) -> Dict[str, Any]:
        """Benchmark backtesting engine performance"""
        print(f"üîÑ Benchmarking backtest engine ({duration}s)...")
        
        try:
            # Import and run simple backtest
            benchmark_start = time.perf_counter()
            
            # Run backtest with timeout
            backtest_cmd = [
                sys.executable, 
                str(self.project_root / "run_backtest.py"),
                "--duration", "1",  # 1 minute test
                "--symbol", "ETH-USDT"
            ]
            
            import subprocess
            result = subprocess.run(
                backtest_cmd, 
                capture_output=True, 
                text=True, 
                timeout=duration,
                cwd=self.project_root
            )
            
            benchmark_duration = time.perf_counter() - benchmark_start
            
            # Parse results if available
            if result.returncode == 0:
                # Look for backtest results
                artifacts_dir = self.project_root / "run_artifacts"
                latest_results = None
                
                if artifacts_dir.exists():
                    result_files = list(artifacts_dir.glob("backtest_results_*.json"))
                    if result_files:
                        latest_file = max(result_files, key=os.path.getctime)
                        try:
                            with open(latest_file, 'r') as f:
                                latest_results = json.load(f)
                        except Exception:
                            pass
                            
                benchmark_results = {
                    'execution_duration_s': benchmark_duration,
                    'backtest_completed': True,
                    'exit_code': result.returncode,
                    'backtest_results': latest_results,
                    'performance_grade': 'EXCELLENT' if result.returncode == 0 and latest_results else 'PARTIAL'
                }
                
                if latest_results:
                    print(f"   Backtest completed successfully")
                    print(f"   Signals: {latest_results.get('signals_generated', 0)}")
                    print(f"   Processing: {latest_results.get('avg_processing_ms', 0):.2f}ms avg")
                else:
                    print(f"   Backtest completed but no detailed results found")
                    
            else:
                benchmark_results = {
                    'execution_duration_s': benchmark_duration,
                    'backtest_completed': False,
                    'exit_code': result.returncode,
                    'error_output': result.stderr,
                    'performance_grade': 'FAILED'
                }
                
                print(f"   ‚ùå Backtest failed with exit code {result.returncode}")
                if result.stderr:
                    print(f"   Error: {result.stderr[:200]}...")
                    
            return benchmark_results
            
        except subprocess.TimeoutExpired:
            print(f"   ‚ö†Ô∏è Backtest timeout after {duration}s")
            return {
                'execution_duration_s': duration,
                'backtest_completed': False,
                'timeout': True,
                'performance_grade': 'TIMEOUT'
            }
        except Exception as e:
            print(f"   ‚ùå Backtest benchmark failed: {e}")
            return {'status': 'failed', 'error': str(e)}
            
    def calculate_resource_summary(self) -> Dict[str, Any]:
        """Calculate resource usage summary"""
        memory_snapshots = self.benchmark_results['resource_monitoring']['memory_snapshots']
        cpu_snapshots = self.benchmark_results['resource_monitoring']['cpu_snapshots']
        
        if not memory_snapshots or not cpu_snapshots:
            return {'status': 'no_data'}
            
        # Memory statistics
        memory_values = [s['rss_mb'] for s in memory_snapshots]
        memory_summary = {
            'peak_memory_mb': max(memory_values),
            'avg_memory_mb': statistics.mean(memory_values),
            'min_memory_mb': min(memory_values),
            'memory_growth_mb': memory_values[-1] - memory_values[0] if len(memory_values) > 1 else 0
        }
        
        # CPU statistics
        cpu_values = [s['cpu_percent'] for s in cpu_snapshots if s['cpu_percent'] is not None]
        if cpu_values:
            cpu_summary = {
                'peak_cpu_percent': max(cpu_values),
                'avg_cpu_percent': statistics.mean(cpu_values),
                'min_cpu_percent': min(cpu_values)
            }
        else:
            cpu_summary = {'status': 'no_cpu_data'}
            
        # Resource targets
        resource_targets = {
            'memory_target_met': memory_summary['peak_memory_mb'] < 450,  # <450MB
            'memory_efficient': memory_summary['avg_memory_mb'] < 400,   # <400MB avg
            'cpu_target_met': cpu_summary.get('peak_cpu_percent', 100) < 85,  # <85%
            'cpu_efficient': cpu_summary.get('avg_cpu_percent', 100) < 70    # <70% avg
        }
        
        return {
            'memory_summary': memory_summary,
            'cpu_summary': cpu_summary,
            'resource_targets': resource_targets,
            'overall_efficiency': sum(resource_targets.values()) / len(resource_targets)
        }
        
    async def run_comprehensive_benchmark(self) -> Dict[str, Any]:
        """Run complete benchmark suite"""
        print(f"üöÄ Starting comprehensive performance benchmark (mode: {self.mode})")
        print(f"   Output directory: {self.output_dir}")
        print()
        
        # Start resource monitoring
        self.start_resource_monitoring()
        
        try:
            # Component benchmarks
            print(f"üèÅ Phase 1: Component Benchmarks")
            indicators_bench = await self.benchmark_indicators()
            self.benchmark_results['benchmarks']['indicators'] = indicators_bench
            
            strategy_bench = await self.benchmark_strategy(30 if self.mode != 'ultra' else 60)
            self.benchmark_results['benchmarks']['strategy'] = strategy_bench
            
            data_fabric_bench = await self.benchmark_data_fabric(20)
            self.benchmark_results['benchmarks']['data_fabric'] = data_fabric_bench
            
            risk_bench = await self.benchmark_risk_manager()
            self.benchmark_results['benchmarks']['risk_manager'] = risk_bench
            
            print(f"\nüèÅ Phase 2: System Integration Benchmark")
            backtest_bench = await self.benchmark_backtest_engine(90 if self.mode == 'ultra' else 60)
            self.benchmark_results['benchmarks']['backtest_engine'] = backtest_bench
            
            # Calculate overall performance summary
            self._calculate_performance_summary()
            
            return self.benchmark_results
            
        finally:
            # Stop resource monitoring
            self.stop_resource_monitoring()
            
    def _calculate_performance_summary(self):
        """Calculate overall performance summary"""
        benchmarks = self.benchmark_results['benchmarks']
        
        # Collect performance grades
        grades = []
        for component, results in benchmarks.items():
            if isinstance(results, dict) and 'performance_grade' in results:
                grade = results['performance_grade']
                grades.append(grade)
            elif isinstance(results, dict) and 'target_performance' in results:
                targets_met = sum(results['target_performance'].values())
                total_targets = len(results['target_performance'])
                grade_mapping = {
                    1.0: 'EXCELLENT',
                    0.75: 'GOOD',
                    0.5: 'ADEQUATE',
                    0.0: 'NEEDS_IMPROVEMENT'
                }
                ratio = targets_met / total_targets
                grade = next((g for r, g in grade_mapping.items() if ratio >= r), 'NEEDS_IMPROVEMENT')
                grades.append(grade)
                
        # Overall grade calculation
        grade_scores = {
            'EXCELLENT': 4,
            'GOOD': 3,
            'ADEQUATE': 2,
            'NEEDS_IMPROVEMENT': 1,
            'FAILED': 0
        }
        
        if grades:
            avg_score = sum(grade_scores.get(g, 0) for g in grades) / len(grades)
            overall_grade = next((g for s, g in sorted(grade_scores.items(), key=lambda x: x[1], reverse=True) if avg_score >= s), 'FAILED')
        else:
            overall_grade = 'UNKNOWN'
            
        # Resource summary
        resource_summary = self.calculate_resource_summary()
        
        self.benchmark_results['performance_summary'] = {
            'overall_grade': overall_grade,
            'component_grades': grades,
            'average_score': avg_score if grades else 0,
            'resource_efficiency': resource_summary.get('overall_efficiency', 0),
            'recommendation': self._get_performance_recommendation(overall_grade, resource_summary)
        }
        
    def _get_performance_recommendation(self, grade: str, resource_summary: Dict[str, Any]) -> str:
        """Get performance recommendation based on results"""
        if grade == 'EXCELLENT':
            return 'System performance excellent - ready for production deployment'
        elif grade == 'GOOD':
            return 'Good performance - minor optimizations recommended'
        elif grade == 'ADEQUATE':
            return 'Adequate performance - optimization recommended before production'
        else:
            return 'Performance needs improvement - address bottlenecks before deployment'
            
    def save_benchmark_results(self) -> Dict[str, Path]:
        """Save comprehensive benchmark results"""
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # Save JSON results
        json_file = self.output_dir / f"benchmark_results_{self.mode}_{timestamp}.json"
        with open(json_file, 'w') as f:
            json.dump(self.benchmark_results, f, indent=2, default=str)
            
        # Generate CSV for analysis
        csv_file = self.output_dir / f"benchmark_metrics_{self.mode}_{timestamp}.csv"
        self._export_metrics_csv(csv_file)
        
        # Generate summary report
        report_file = self.output_dir / f"benchmark_report_{self.mode}_{timestamp}.md"
        report_content = self._generate_benchmark_report()
        with open(report_file, 'w') as f:
            f.write(report_content)
            
        print(f"\nüíæ Benchmark results saved:")
        print(f"   JSON: {json_file}")
        print(f"   CSV: {csv_file}")
        print(f"   Report: {report_file}")
        
        return {
            'json': json_file,
            'csv': csv_file,
            'report': report_file
        }
        
    def _export_metrics_csv(self, csv_file: Path):
        """Export metrics as CSV for analysis"""
        try:
            import csv
            
            with open(csv_file, 'w', newline='') as f:
                writer = csv.writer(f)
                
                # Header
                writer.writerow([
                    'timestamp', 'component', 'metric', 'value', 'unit', 'target_met'
                ])
                
                # Write benchmark data
                for component, results in self.benchmark_results['benchmarks'].items():
                    if isinstance(results, dict):
                        timestamp = self.benchmark_results['timestamp']
                        
                        # Extract key metrics
                        if 'avg_update_time_us' in results:
                            writer.writerow([
                                timestamp, component, 'avg_latency_us', 
                                results['avg_update_time_us'], 'microseconds',
                                results.get('target_performance', {}).get('latency_target_met', False)
                            ])
                            
                        if 'operations_per_second' in results:
                            writer.writerow([
                                timestamp, component, 'throughput_ops_per_sec',
                                results['operations_per_second'], 'operations/second', 
                                results.get('target_performance', {}).get('throughput_target_met', False)
                            ])
                            
        except Exception as e:
            print(f"   ‚ö†Ô∏è CSV export failed: {e}")
            
    def _generate_benchmark_report(self) -> str:
        """Generate comprehensive benchmark report"""
        summary = self.benchmark_results['performance_summary']
        resource_summary = self.calculate_resource_summary()
        
        report = f"""
# ‚ö° SUPREME SYSTEM V5 - PERFORMANCE BENCHMARK REPORT

**Generated:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S UTC')}
**Mode:** {self.mode.upper()}
**Overall Grade:** {summary['overall_grade']}
**Resource Efficiency:** {resource_summary.get('overall_efficiency', 0):.1%}

## üèÜ PERFORMANCE SUMMARY

**Overall Grade:** {summary['overall_grade']}
**Recommendation:** {summary['recommendation']}

### Component Performance
"""
        
        for i, grade in enumerate(summary.get('component_grades', [])):
            components = ['Indicators', 'Strategy', 'Data Fabric', 'Risk Manager', 'Backtest Engine']
            component = components[i] if i < len(components) else f'Component {i+1}'
            report += f"- **{component}:** {grade}\n"
            
        # Resource usage summary
        if resource_summary.get('memory_summary'):
            mem = resource_summary['memory_summary']
            cpu = resource_summary['cpu_summary']
            
            report += f"""

## üìä RESOURCE USAGE ANALYSIS

### Memory Usage
- **Peak Usage:** {mem['peak_memory_mb']:.1f} MB
- **Average Usage:** {mem['avg_memory_mb']:.1f} MB
- **Memory Growth:** {mem['memory_growth_mb']:.1f} MB
- **Target Compliance:** {'\u2705 PASS' if mem['peak_memory_mb'] < 450 else '\u274c FAIL'} (<450MB)

### CPU Usage
- **Peak CPU:** {cpu.get('peak_cpu_percent', 0):.1f}%
- **Average CPU:** {cpu.get('avg_cpu_percent', 0):.1f}%
- **Target Compliance:** {'\u2705 PASS' if cpu.get('peak_cpu_percent', 100) < 85 else '\u274c FAIL'} (<85%)
"""
            
        # Detailed component results
        report += "\n## üîç DETAILED COMPONENT ANALYSIS\n"
        
        for component, results in self.benchmark_results['benchmarks'].items():
            if isinstance(results, dict) and 'status' not in results:
                report += f"\n### {component.replace('_', ' ').title()}\n"
                
                # Key metrics
                if 'avg_update_time_us' in results:
                    report += f"- **Average Latency:** {results['avg_update_time_us']:.0f}Œºs\n"
                if 'operations_per_second' in results:
                    report += f"- **Throughput:** {results['operations_per_second']:.0f} ops/sec\n"
                if 'success_rate' in results:
                    report += f"- **Success Rate:** {results['success_rate']*100:.1f}%\n"
                    
        report += f"""

## üéÜ BENCHMARK CONCLUSIONS

### System Readiness
- **Production Ready:** {'YES' if summary['overall_grade'] in ['EXCELLENT', 'GOOD'] else 'NEEDS_WORK'}
- **Performance Acceptable:** {'YES' if summary.get('average_score', 0) >= 2.5 else 'NO'}
- **Resource Compliant:** {'YES' if resource_summary.get('overall_efficiency', 0) >= 0.75 else 'NO'}

### Next Steps
1. **Address Performance Issues** - Focus on components with 'NEEDS_IMPROVEMENT' grade
2. **Resource Optimization** - Optimize memory/CPU usage if targets not met
3. **Regression Testing** - Use this benchmark as baseline for future comparisons
4. **Production Deployment** - System ready if all grades 'GOOD' or better

---
*Generated by Agentic Performance Benchmark Suite*
"""
        
        return report

async def main():
    """Main benchmark execution"""
    parser = argparse.ArgumentParser(description='Agentic Performance Benchmark Suite')
    parser.add_argument('--mode', choices=['standard', 'enhanced', 'ultra'], 
                       default='standard', help='Benchmark intensity mode')
    parser.add_argument('--component', type=str, help='Benchmark specific component only')
    parser.add_argument('--duration', type=int, default=30, help='Benchmark duration per component')
    parser.add_argument('--output', type=str, default='run_artifacts/benchmarks', help='Output directory')
    parser.add_argument('--regression-test', type=str, help='Compare against baseline JSON file')
    
    args = parser.parse_args()
    
    print(f"Configuration:")
    print(f"  Mode: {args.mode}")
    print(f"  Component: {args.component or 'All components'}")
    print(f"  Duration: {args.duration}s per component")
    print(f"  Output: {args.output}")
    print()
    
    # Initialize benchmark suite
    suite = PerformanceBenchmarkSuite(args.mode, args.output)
    
    try:
        if args.component:
            print(f"üéØ Running single component benchmark: {args.component}")
            # Single component benchmark logic would go here
            results = await suite.run_comprehensive_benchmark()  # Fallback to full suite
        else:
            print(f"üöÄ Running comprehensive benchmark suite")
            results = await suite.run_comprehensive_benchmark()
            
        # Save results
        files_saved = suite.save_benchmark_results()
        
        # Print final summary
        summary = results['performance_summary']
        print(f"\nüéÜ BENCHMARK COMPLETE!")
        print(f"   Overall Grade: {summary['overall_grade']}")
        print(f"   Resource Efficiency: {summary.get('resource_efficiency', 0):.1%}")
        print(f"   Recommendation: {summary['recommendation']}")
        
        # Determine exit code based on performance
        if summary['overall_grade'] in ['EXCELLENT', 'GOOD']:
            print(f"\n‚úÖ Performance benchmarks PASSED")
            return 0
        elif summary['overall_grade'] in ['ADEQUATE']:
            print(f"\n‚ö†Ô∏è Performance benchmarks MARGINAL")
            return 1
        else:
            print(f"\n‚ùå Performance benchmarks FAILED")
            return 2
            
    except KeyboardInterrupt:
        print(f"\nüõÑ Benchmark interrupted by user")
        return 0
    except Exception as e:
        print(f"\n‚ùå Benchmark failed: {e}")
        print(f"Traceback: {tracemalloc.format_tb(e.__traceback__) if hasattr(e, '__traceback__') else 'N/A'}")
        return 1
    finally:
        tracemalloc.stop()

if __name__ == "__main__":
    import asyncio
    exit_code = asyncio.run(main())
    
    if exit_code == 0:
        print(f"\nüéâ Benchmark suite completed successfully!")
    elif exit_code == 1:
        print(f"\n‚ö†Ô∏è Benchmark suite completed with warnings.")
    else:
        print(f"\n‚ùå Benchmark suite failed - performance issues detected.")
        
    sys.exit(exit_code)